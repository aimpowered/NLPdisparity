{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import copy\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from docx import Document\n",
    "import os\n",
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Loader for benchmarking datasets to ensure universal formatting. To be used in conjunction with DyslexiaInjector.\n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    path: str\n",
    "        Path to csv, txt or docx file of the data. In the case of CSV there should only be 1 column\n",
    "    data: list\n",
    "        A list of striings\n",
    "    dataset_name: str\n",
    "        Name of the dataset that is used when saving the data\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    save_as_txt(path):\n",
    "        Saves data as a text file to specified path\n",
    "    save_as_csv(path):\n",
    "        Saves data as a csv file to specified path\n",
    "    get_data():\n",
    "        Returns data\n",
    "    create_deepcopy():\n",
    "        Returns a deep copy of the class instance\n",
    "    get_name():\n",
    "        returns name of the dataset (dataset_name)\n",
    "    \n",
    "    Usage\n",
    "    -------\n",
    "    >>> from datasets import load_dataset\n",
    "    >>> from DataLoader import DataLoader\n",
    "    >>> dataset_wmt_enfr = load_dataset(\"wmt14\",'fr-en', split='test')\n",
    "    >>> to_translate = []\n",
    "    >>> for i in range(len(dataset_wmt_enfr)):\n",
    "    >>>     to_translate.append(dataset_wmt_enfr[i]['translation']['en'])\n",
    "    >>> loader = DataLoader(data=to_translate, dataset_name=\"wmt14_enfr\")\n",
    "    >>> loader.save_as_txt(\"wmt14_enfr.txt\")\n",
    "    We can also use the text file to create a new DataLoader instance\n",
    "    >>> loader2 = DataLoader(path=\"wmt14_enfr.txt\", dataset_name=\"wmt14_enfr\")\n",
    "    \"\"\"\n",
    "    # Constructor\n",
    "    def __init__(self, path=None, data=None, dataset_name=\"\"):\n",
    "        self.dataset_name = dataset_name\n",
    "        if data is None and path is not None:\n",
    "            #check path to see if file is txt or csv\n",
    "            file_type = path.split(\".\")[-1]\n",
    "            if file_type == \"txt\":\n",
    "                self.data = self.parse_txt(path)\n",
    "                self.data = [self.fix_format(sentence) for sentence in self.data]\n",
    "            elif file_type == \"csv\":\n",
    "                self.data = pd.read_csv(path, header=None)\n",
    "                self.data = self.data[0].tolist()\n",
    "                #fix any formatting issues\n",
    "                self.data = [self.fix_format(sentence) for sentence in self.data]\n",
    "            elif file_type == \"docx\":\n",
    "                doc = Document(path)\n",
    "                self.data = [self.fix_format(paragraph.text) for paragraph in doc.paragraphs]\n",
    "            else:\n",
    "                raise Exception(\"Invalid file type\")\n",
    "        elif data is not None:\n",
    "            #check if data is a list or a df\n",
    "            if isinstance(data, list):\n",
    "                #format each sentence in data\n",
    "                self.data = [self.fix_format(sentence) for sentence in data]\n",
    "            else:\n",
    "                raise Exception(\"Invalid data type, please pass in a list of sentences\")\n",
    "        else:\n",
    "            raise Exception(\"Please pass in a path or data\")\n",
    "\n",
    "    def parse_txt(self, path):\n",
    "        output = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                output.append(self.fix_format(line))\n",
    "        return output\n",
    "                \n",
    "    def fix_format(self, sentence):\n",
    "        #remove spacing before punctuation\n",
    "        sentence = re.sub(r'\\s([?.!,\"](?:\\s|$))', r'\\1', sentence)\n",
    "        #replace any double spaces with single space\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "        #remove any leading or trailing spaces\n",
    "        sentence = sentence.strip()\n",
    "        #make all quotes (german and french) english double quotes\n",
    "        sentence = re.sub(r'«|»|„|“', '\"', sentence)\n",
    "        #make all single quotes english single quotes\n",
    "        sentence = re.sub(r'‘|’', \"'\", sentence)\n",
    "        #make all french guillemets english double quotes\n",
    "        sentence = re.sub(r'‹|›', '\"', sentence)\n",
    "        #if sentence begins and ends with quotes and there are only two, remove them\n",
    "        if sentence[0] == '\"' and sentence[-1] == '\"' and sentence.count('\"') == 2:\n",
    "            sentence = sentence[1:-1]\n",
    "        elif sentence[0] == \"'\" and sentence[-1] == \"'\" and sentence.count(\"'\") == 2:\n",
    "            sentence = sentence[1:-1]\n",
    "        return sentence\n",
    "\n",
    "    def save_as_txt(self, path):\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for sentence in self.data:\n",
    "                f.write(f\"{sentence}\\n\")\n",
    "        print(f\"Saved {self.dataset_name} to {path}\")\n",
    "        return\n",
    "    \n",
    "    def save_as_csv(self, path):\n",
    "        df = pd.DataFrame(self.data)\n",
    "        df.to_csv(path, index=False, header=False, encoding='utf-8')\n",
    "        print(f\"Saved {self.dataset_name} to {path}\")\n",
    "        return\n",
    "    \n",
    "    def save_as_docx(self, path):\n",
    "        document = Document()\n",
    "        for sentence in self.data:\n",
    "            document.add_paragraph(sentence)\n",
    "        document.save(path)\n",
    "        print(f\"Saved {self.dataset_name} to {path}\")\n",
    "        return\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "\n",
    "    def create_deepcopy(self):\n",
    "        return DataLoader(data=copy.deepcopy(self.data), dataset_name=self.dataset_name)\n",
    "        \n",
    "    def get_name(self):\n",
    "        return self.dataset_name\n",
    "\n",
    "    def get_number_of_sentences(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get_number_of_words(self):\n",
    "        return sum([len(sentence.split()) for sentence in self.data])\n",
    "    \n",
    "    def get_number_of_letters(self):\n",
    "        #need to ensure we only count letters and not punctuation\n",
    "        return sum([len(re.sub(r'[^\\w\\s]','',sentence)) for sentence in self.data])\n",
    "\n",
    "    def edit_distance(reference_sentence, sentence):\n",
    "        \"\"\"\n",
    "        Returns the number of edits required to transform reference_sentence into sentence at word level\n",
    "        edits include insertions, deletions and substitutions\n",
    "        based on levenshtein distance\n",
    "        also returns a dictionary of substitutions, insertions and deletions\n",
    "        \"\"\"\n",
    "        substitutions = 0\n",
    "        insertions = 0\n",
    "        deletions = 0\n",
    "        substitution_dict = {}\n",
    "        insertion_dict = {}\n",
    "        deletion_dict = {}\n",
    "        #remove punctuation and split into words\n",
    "        sentence = re.sub(r'[^\\w\\s]','',sentence).lower().split()\n",
    "        reference_sentence = re.sub(r'[^\\w\\s]','',reference_sentence).lower().split()\n",
    "        #create matrix\n",
    "        matrix = np.zeros((len(reference_sentence)+1,len(sentence)+1))\n",
    "        #fill in first row and column\n",
    "        for i in range(len(reference_sentence)+1):\n",
    "            matrix[i][0] = i\n",
    "        for j in range(len(sentence)+1):\n",
    "            matrix[0][j] = j\n",
    "        #fill in rest of matrix\n",
    "        for i in range(1,len(reference_sentence)+1):\n",
    "            for j in range(1,len(sentence)+1):\n",
    "                if sentence[j-1] == reference_sentence[i-1]:\n",
    "                    matrix[i][j] = matrix[i-1][j-1]\n",
    "                else:\n",
    "                    matrix[i][j] = min(matrix[i-1][j-1], matrix[i-1][j], matrix[i][j-1])+1\n",
    "        #backtrack to find edits\n",
    "        i = len(reference_sentence)\n",
    "        j = len(sentence)\n",
    "        while i > 0 and j > 0:\n",
    "            if sentence[j-1] == reference_sentence[i-1]:\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            else:\n",
    "                if matrix[i][j] == matrix[i-1][j-1]+1:\n",
    "                    substitutions += 1\n",
    "                    if reference_sentence[i-1] not in substitution_dict:\n",
    "                        substitution_dict[reference_sentence[i-1]] = [sentence[j-1],1]\n",
    "                    else:\n",
    "                        for item in substitution_dict[reference_sentence[i-1]]:\n",
    "                            if item[0] == sentence[j-2]:\n",
    "                                item[1] += 1\n",
    "                                break\n",
    "                    i -= 1\n",
    "                    j -= 1\n",
    "                elif matrix[i][j] == matrix[i-1][j]+1:\n",
    "                    deletions += 1\n",
    "                    if reference_sentence[i-1] not in deletion_dict:\n",
    "                        deletion_dict[reference_sentence[i-1]] = 1\n",
    "                    else:\n",
    "                        deletion_dict[reference_sentence[i-1]] += 1\n",
    "                    i -= 1\n",
    "                elif matrix[i][j] == matrix[i][j-1]+1:\n",
    "                    insertions += 1\n",
    "                    if sentence[j-1] not in insertion_dict:\n",
    "                        insertion_dict[sentence[j-1]] = 1\n",
    "                    else:\n",
    "                        insertion_dict[sentence[j-1]] += 1\n",
    "                    j -= 1\n",
    "        while i > 0:\n",
    "            deletions += 1\n",
    "            if reference_sentence[i-1] not in deletion_dict:\n",
    "                deletion_dict[reference_sentence[i-1]] = 1\n",
    "            else:\n",
    "                deletion_dict[reference_sentence[i-1]] += 1\n",
    "            i -= 1\n",
    "        while j > 0:\n",
    "            insertions += 1\n",
    "            if sentence[j-1] not in insertion_dict:\n",
    "                insertion_dict[sentence[j-1]] = 1\n",
    "            else:\n",
    "                insertion_dict[sentence[j-1]] += 1\n",
    "            j -= 1\n",
    "        return substitutions, insertions, deletions, substitution_dict, insertion_dict, deletion_dict\n",
    "        \n",
    "    def get_edit_distance(self, reference):\n",
    "        \"\"\"\n",
    "        Returns the number of edits required to transform data into reference at word level\n",
    "        edits include insertions, deletions and substitutions\n",
    "        based on levenshtein distance\n",
    "        \"\"\"\n",
    "        if type(reference) == list:\n",
    "            substitutions = 0\n",
    "            insertions = 0\n",
    "            deletions = 0\n",
    "            for i in range(len(self.data)):\n",
    "                sub, ins, dele = DataLoader.edit_distance(self.data[i], reference[i])\n",
    "                substitutions += sub\n",
    "                insertions += ins\n",
    "                deletions += dele\n",
    "            return substitutions, insertions, deletions\n",
    "        elif type(reference) == DataLoader:\n",
    "            return self.get_edit_distance(reference.get_data())\n",
    "        else:\n",
    "            raise Exception(\"Invalid reference type, please pass in a list or DataLoader instance\")\n",
    "    \n",
    "    def get_bleue_score(self, reference):\n",
    "        #returns bleu score of the data against a reference\n",
    "        bleu = evaluate.load(\"bleu\")\n",
    "        if type(reference) == list:\n",
    "            return bleu.compute(predictions=self.data, references=reference)\n",
    "        elif type(reference) == DataLoader:\n",
    "            return bleu.compute(predictions=self.data, references=reference.get_data())\n",
    "        else:\n",
    "            raise Exception(\"Invalid reference type, please pass in a list or DataLoader instance\")\n",
    "\n",
    "    def get_wer(self, reference):\n",
    "        #returns wer score of the data against a reference\n",
    "        wer = evaluate.load(\"wer\")\n",
    "        if type(reference) == list:\n",
    "            return wer.compute(predictions=self.data, references=reference)\n",
    "        elif type(reference) == DataLoader:\n",
    "            return wer.compute(predictions=self.data, references=reference.get_data())\n",
    "        else:\n",
    "            raise Exception(\"Invalid reference type, please pass in a list or DataLoader instance\")\n",
    "\n",
    "    def get_bleurt_score(self, reference):\n",
    "        #returns bleurt score of the data against a reference\n",
    "        bleurt = evaluate.load(\"bleurt\")\n",
    "        if type(reference) == list:\n",
    "            return bleurt.compute(predictions=self.data, references=reference)\n",
    "        elif type(reference) == DataLoader:\n",
    "            return bleurt.compute(predictions=self.data, references=reference.get_data())\n",
    "        else:\n",
    "            raise Exception(\"Invalid reference type, please pass in a list or DataLoader instance\")\n",
    "            \n",
    "    def get_bert_score(self, reference):\n",
    "        #returns bert score of the data against a reference\n",
    "        bert = evaluate.load(\"bertscore\")\n",
    "        if type(reference) == list:\n",
    "            return bert.compute(predictions=self.data, references=reference, lang=\"fr\")\n",
    "        elif type(reference) == DataLoader:\n",
    "            return bert.compute(predictions=self.data, references=reference.get_data(), lang=\"fr\")\n",
    "        else:\n",
    "            raise Exception(\"Invalid reference type, please pass in a list or DataLoader instance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0, 0, {'jumped': ['jumps', 1]}, {}, {})\n"
     ]
    }
   ],
   "source": [
    "#test edit distance method\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "reference_sentence = \"The quick brown fox jumped over the lazy dog\"\n",
    "print(DataLoader.edit_distance(reference_sentence, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt14 (C:/Users/gpric/.cache/huggingface/datasets/wmt14/fr-en/1.0.0/2de185b074515e97618524d69f5e27ee7545dcbed4aa9bc1a4235710ffca33f4)\n"
     ]
    }
   ],
   "source": [
    "#need to loop through file directory\n",
    "aws_data_v1 = []\n",
    "aws_data_v2 = []\n",
    "for filename in os.listdir(\"to_test/v1/aws\"):\n",
    "    temp = DataLoader(path=\"to_test/v1/aws/\"+filename, dataset_name=\"aws_\"+filename)\n",
    "    aws_data_v1.append(temp)\n",
    "\n",
    "for filename in os.listdir(\"to_test/v2/aws\"):\n",
    "    temp = DataLoader(path=\"to_test/v2/aws/\"+filename, dataset_name=\"aws_\"+filename)\n",
    "    aws_data_v2.append(temp)\n",
    "\n",
    "azure_data_v1 = []\n",
    "for filename in os.listdir(\"to_test/v1/azure\"):\n",
    "    temp = DataLoader(path=\"to_test/v1/azure/\"+filename, dataset_name=\"azure_\"+filename)\n",
    "    azure_data_v1.append(temp)\n",
    "\n",
    "azure_data_v2 = []\n",
    "for filename in os.listdir(\"to_test/v2/azure\"):\n",
    "    temp = DataLoader(path=\"to_test/v2/azure/\"+filename, dataset_name=\"azure_\"+filename)\n",
    "    azure_data_v2.append(temp)\n",
    "\n",
    "google_data_v1 = []\n",
    "for filename in os.listdir(\"to_test/v1/google\"):\n",
    "    temp = DataLoader(path=\"to_test/v1/google/\"+filename, dataset_name=\"google_\"+filename)\n",
    "    google_data_v1.append(temp)\n",
    "\n",
    "google_data_v2 = []\n",
    "for filename in os.listdir(\"to_test/v2/google\"):\n",
    "    temp = DataLoader(path=\"to_test/v2/google/\"+filename, dataset_name=\"google_\"+filename)\n",
    "    google_data_v2.append(temp)\n",
    "\n",
    "gpt_data_v1 = []\n",
    "for filename in os.listdir(\"to_test/v1/gpt\"):\n",
    "    temp = DataLoader(path=\"to_test/v1/gpt/\"+filename, dataset_name=\"gpt_\"+filename)\n",
    "    gpt_data_v1.append(temp)\n",
    "\n",
    "gpt_data_v2 = []\n",
    "for filename in os.listdir(\"to_test/v2/gpt\"):\n",
    "    temp = DataLoader(path=\"to_test/v2/gpt/\"+filename, dataset_name=\"gpt_\"+filename)\n",
    "    gpt_data_v2.append(temp)\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset_wmt_enfr = load_dataset(\"wmt14\",'fr-en', split='test')\n",
    "to_translate_wmt14_en = []\n",
    "reference_wmt14_fr = []\n",
    "\n",
    "for i in range(len(dataset_wmt_enfr)):\n",
    "    to_translate_wmt14_en.append(dataset_wmt_enfr[i]['translation']['en'])\n",
    "    reference_wmt14_fr.append(dataset_wmt_enfr[i]['translation']['fr'])\n",
    "\n",
    "reference_corpus_fr = DataLoader(data=reference_wmt14_fr, dataset_name=\"wmt14_fr\")\n",
    "reference_corpus_en = DataLoader(data=to_translate_wmt14_en, dataset_name=\"wmt14_en\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
