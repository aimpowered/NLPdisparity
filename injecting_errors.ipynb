{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spectacular Wingsuit Jump Over Bogota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sportsman Jhonathan Florez jumped from a helic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wearing a wingsuit, he flew past over the famo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A black box in your car?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As America's road planners struggle to find th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0              Spectacular Wingsuit Jump Over Bogota\n",
       "1  Sportsman Jhonathan Florez jumped from a helic...\n",
       "2  Wearing a wingsuit, he flew past over the famo...\n",
       "3                           A black box in your car?\n",
       "4  As America's road planners struggle to find th..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_wmt14_en = pd.read_csv('data/wmt14_english_data.csv', header=None)\n",
    "df_wmt14_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Obama receives Netanyahu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The relationship between Obama and Netanyahu i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The two wanted to talk about the implementatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The meeting was also planned to cover the conf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Relations between Obama and Netanyahu have bee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                           Obama receives Netanyahu\n",
       "1  The relationship between Obama and Netanyahu i...\n",
       "2  The two wanted to talk about the implementatio...\n",
       "3  The meeting was also planned to cover the conf...\n",
       "4  Relations between Obama and Netanyahu have bee..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wmt16_en = pd.read_csv('data/wmt16_english_data.csv', header=None)\n",
    "df_wmt16_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retreived from https://github.com/kjanjua26/Pyphones/tree/master\n",
    "Wen have modified this code for our needs\n",
    "Python wrapper for the website: https://www.homophone.com/\n",
    "Gets the homophones of a word.\n",
    "\"\"\"\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, List\n",
    "import re\n",
    "\n",
    "class Pyphones:\n",
    "    \n",
    "    def __init__(self, word):\n",
    "        self.word = word\n",
    "        self.url = \"https://www.homophone.com/search?page={}&type=&q={}\"\n",
    "        self.homophones = {self.word: []}\n",
    "        \n",
    "    def get_the_page(self, page_no=1):\n",
    "        \"\"\"\n",
    "        Get the page content.\n",
    "\n",
    "        Returns\n",
    "            str: the content of the page.\n",
    "        \"\"\"\n",
    "        url = self.url.format(page_no, self.word)\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        return soup\n",
    "\n",
    "    def get_the_page_nos(self):\n",
    "        \"\"\"\n",
    "        Get the total number of pages\n",
    "\n",
    "        Returns\n",
    "            int: the total number of the pages.\n",
    "        \"\"\"\n",
    "        soup = self.get_the_page()\n",
    "        pages = soup.find_all('div', attrs={'class':'col-sm-9'})\n",
    "        total_pages = pages[0].find('h5').text.split('/')[-1].strip()\n",
    "        return int(total_pages)\n",
    "\n",
    "    def get_the_homophones(self):\n",
    "        \"\"\"\n",
    "        Get the homophones of the word.\n",
    "\n",
    "        Returns\n",
    "            dict: {word: [list_of_homophones]} against each word.\n",
    "        \"\"\"\n",
    "        total_pages = self.get_the_page_nos()\n",
    "        for ix in range(total_pages):\n",
    "            page_no = ix + 1\n",
    "            soup = self.get_the_page(page_no)\n",
    "            raw_homophones = soup.find_all('div', attrs={'class': 'well well-lg'})\n",
    "            for elem in range(len(raw_homophones)):\n",
    "                raw_homophones_2 = raw_homophones[elem].find_all('a', attrs={'class': 'btn word-btn'})\n",
    "                list_of_homophones = list(raw_homophones_2)\n",
    "                if any(list_of_homophones):\n",
    "                    local_homophones = []\n",
    "                    for tag_of_homophone in list_of_homophones:\n",
    "                        homophone = tag_of_homophone.text\n",
    "                        local_homophones.append(homophone)\n",
    "                    self.homophones[self.word].append(local_homophones)\n",
    "\n",
    "        #we want to modify the ouput to only the list within the list that the firsrt word is the same as the input word. \n",
    "        for key in self.homophones.keys():\n",
    "            for i in range(len(self.homophones[key])):\n",
    "                if self.homophones[key][i][0] == key:\n",
    "                    return self.homophones[key].pop(i)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there', 'their', \"they're\"]\n"
     ]
    }
   ],
   "source": [
    "out = Pyphones(\"there\")\n",
    "homophones = out.get_the_homophones()\n",
    "print(homophones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14899\n",
      "['spectacular', 'wingsuit', 'jump', 'over', 'bogota', 'sportsman', 'jhonathan', 'florez', 'jumped', 'from', 'a', 'helicopter', 'above', 'the', 'capital']\n"
     ]
    }
   ],
   "source": [
    "#lets make a dictornay of all unique words in both datasets and their homophones\n",
    "#we will use the wmt14 dataset nad wmt16 as the base\n",
    "\n",
    "#first we will make a dict of all unique words in both datasets\n",
    "\n",
    "words = {}\n",
    "\n",
    "for i in range(len(df_wmt14_en)):\n",
    "    for j in range(len(df_wmt14_en.iloc[i,0].split())):\n",
    "        #make sure to lower case all words and remove punctuation at the end of the word and the beginning\n",
    "        words[df_wmt14_en.iloc[i,0].split()[j].lower().strip('\".,?!:;()')] = []       \n",
    "\n",
    "for i in range(len(df_wmt16_en)):\n",
    "    for j in range(len(df_wmt16_en.iloc[i,0].split())):\n",
    "        #make sure to lower case all words and remove punctuation at the end of the word and the beginning\n",
    "        words[df_wmt16_en.iloc[i,0].split()[j].lower().strip('\".,?!:;()')] = []   \n",
    "\n",
    "print(len(words))\n",
    "#print some keys of words\n",
    "print(list(words.keys())[0:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'uh']\n"
     ]
    }
   ],
   "source": [
    "out = Pyphones(\"a\").get_the_homophones()\n",
    "\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----0/14899----\n",
      "Homophones for over: []\n",
      "Homophones for a: ['a', 'uh']\n",
      "Homophones for the: ['the', 'thee']\n",
      "Homophones for capital: ['capital', 'capitol']\n",
      "Homophones for of: []\n",
      "Homophones for on: []\n",
      "Homophones for he: []\n",
      "Homophones for flew: ['flew', 'flu', 'flue']\n",
      "Homophones for past: ['past', 'passed']\n",
      "Homophones for at: []\n",
      "Homophones for is: []\n",
      "Homophones for an: []\n",
      "Homophones for and: []\n",
      "Homophones for there: ['there', 'their', \"they're\"]\n",
      "Homophones for to: ['to', 'too', 'two']\n",
      "Homophones for his: []\n",
      "Homophones for box: ['box', 'bocks']\n",
      "Homophones for in: ['in', 'inn']\n",
      "Homophones for your: ['your', 'yore', \"you're\"]\n",
      "Homophones for car: []\n",
      "Homophones for as: ['as', 'ass', 'asse']\n",
      "Homophones for road: ['road', 'rode', 'rowed']\n",
      "Homophones for find: ['find', 'fined']\n",
      "Homophones for cash: ['cash', 'cache']\n",
      "Homophones for are: ['are', 'air', 'aire', 'ayre', 'ere', 'err', 'eyre', 'heir']\n",
      "Homophones for see: ['see', 'c', 'cee', 'sea']\n",
      "Homophones for by: ['by', 'bye', 'bi', 'buy']\n",
      "Homophones for which: ['which', 'wich', 'witch']\n",
      "Homophones for track: []\n",
      "Homophones for center: []\n",
      "Homophones for state: []\n",
      "Homophones for for: ['for', 'fore', 'four']\n",
      "Homophones for has: []\n",
      "Homophones for intense: ['intense', 'intents']\n",
      "----100/14899----\n",
      "Homophones for have: ['have', 'halve']\n",
      "Homophones for allow: []\n",
      "Homophones for use: ['use', 'ewes', 'yews']\n",
      "Homophones for you: ['you', 'ewe', 'yew']\n",
      "Homophones for where: ['where', 'ware', 'wear', 'weir']\n",
      "Homophones for -: []\n",
      "Homophones for up: []\n",
      "Homophones for tax: ['tax', 'tacks']\n",
      "Homophones for bill: []\n",
      "Homophones for tea: ['tea', 't', 'tee', 'ti']\n",
      "Homophones for too: ['too', 'to', 'two']\n",
      "Homophones for raising: ['raising', 'rasing', 'razing']\n",
      "Homophones for while: ['while', 'wile']\n",
      "Homophones for can't: [\"can't\", 'cant']\n",
      "Homophones for whether: ['whether', 'weather', 'wether']\n",
      "Homophones for not: ['not', 'knot']\n",
      "Homophones for waiting: ['waiting', 'weighting']\n",
      "Homophones for they: []\n",
      "Homophones for how: []\n",
      "Homophones for can: []\n",
      "Homophones for pay: []\n",
      "Homophones for per: ['per', 'purr']\n",
      "Homophones for roll: ['roll', 'role']\n",
      "Homophones for already: []\n",
      "Homophones for some: ['some', 'sum']\n",
      "Homophones for must: ['must', 'mussed']\n",
      "Homophones for our: ['our', 'hour']\n",
      "Homophones for it: []\n",
      "Homophones for we: ['we', 'wee', 'whee']\n",
      "Homophones for might: ['might', 'mite']\n",
      "Homophones for choose: ['choose', 'chews']\n",
      "Homophones for do: ['do', 'dew', 'due']\n",
      "Homophones for said: ['said', 'sed']\n",
      "Homophones for be: ['be', 'bee', 'b']\n",
      "----200/14899----\n",
      "Homophones for trust: ['trust', 'trussed']\n",
      "Homophones for with: []\n",
      "Homophones for gas: []\n",
      "Homophones for buy: ['buy', 'bi', 'by', 'bye']\n",
      "Homophones for much: ['much', 'mutch']\n",
      "Homophones for used: []\n",
      "Homophones for get: []\n",
      "Homophones for cents: ['cents', 'scents', 'cense', 'sense']\n",
      "Homophones for loath: ['loath', 'loathe']\n",
      "Homophones for raise: ['raise', 'rase', 'rays', 'raze', 'res']\n",
      "Homophones for one: ['one', 'won']\n",
      "Homophones for penny: ['penny', 'penni']\n",
      "Homophones for when: ['when', 'wen']\n",
      "Homophones for high: ['high', 'heigh', 'hi', 'hie']\n",
      "Homophones for lee: ['lee', 'lea', 'li']\n",
      "Homophones for put: []\n",
      "Homophones for out: []\n",
      "Homophones for call: ['call', 'caul', 'col']\n",
      "Homophones for user: []\n",
      "Homophones for fee: []\n",
      "Homophones for no: ['no', 'know']\n",
      "Homophones for urban: []\n",
      "Homophones for ways: ['ways', 'weighs']\n",
      "Homophones for devise: []\n",
      "Homophones for meet: ['meet', 'meat', 'mete']\n",
      "Homophones for laid: ['laid', 'lade', 'layed']\n",
      "Homophones for laws: []\n",
      "Homophones for but: ['but', 'butt']\n",
      "Homophones for rep: []\n",
      "Homophones for house: ['house', \"how's\", 'hows']\n",
      "Homophones for sees: ['sees', \"c's\", 'cees', 'seas', 'seize']\n",
      "Homophones for free: []\n",
      "----300/14899----\n",
      "Homophones for hole: ['hole', 'whole']\n",
      "Homophones for vice: ['vice', 'vise']\n",
      "Homophones for what: ['what', 'watt', 'wot']\n",
      "Homophones for two: ['two', 'to', 'too']\n",
      "Homophones for who: ['who', 'hoo']\n",
      "Homophones for senate: ['senate', 'sennet', 'sennit']\n",
      "Homophones for would: ['would', 'wood']\n",
      "Homophones for whose: ['whose', \"who's\"]\n",
      "Homophones for or: ['or', 'ore', 'oar']\n",
      "Homophones for their: ['their', 'there', \"they're\"]\n",
      "Homophones for own: []\n",
      "Homophones for will: []\n",
      "Homophones for fees: ['fees', 'feaze', 'feeze']\n",
      "Homophones for new: ['new', 'knew', 'gnu', 'nu']\n",
      "----400/14899----\n",
      "Homophones for go: []\n",
      "Homophones for hit: []\n",
      "Homophones for were: ['were', 'whir']\n",
      "Homophones for able: ['able', 'Abel']\n",
      "Homophones for big: []\n",
      "Homophones for khan: ['khan', 'con', 'conn']\n",
      "Homophones for was: []\n",
      "Homophones for its: ['its', \"it's\"]\n",
      "Homophones for turn: ['turn', 'tern', 'terne']\n",
      "Homophones for need: ['need', 'kneed', 'knead']\n",
      "Homophones for build: ['build', 'billed']\n",
      "Homophones for now: []\n",
      "Homophones for time: ['time', 'thyme']\n",
      "Homophones for if: []\n",
      "Homophones for led: ['led', 'lead']\n",
      "Homophones for break: ['break', 'brake']\n",
      "Homophones for emerging: ['emerging', 'immerging']\n",
      "Homophones for based: ['based', 'baste']\n",
      "Homophones for been: ['been', 'bean']\n",
      "Homophones for lot: []\n",
      "Homophones for less: ['less', 'loess']\n",
      "----500/14899----\n",
      "Homophones for all: ['all', 'awl']\n",
      "Homophones for residents: []\n",
      "Homophones for other: []\n",
      "Homophones for sell: ['sell', 'cel', 'cell']\n",
      "Homophones for meter: ['meter', 'meeter']\n",
      "Homophones for talk: []\n",
      "Homophones for san: []\n",
      "Homophones for bay: ['bay', 'bey']\n",
      "Homophones for say: ['say', 'se']\n",
      "Homophones for very: []\n",
      "Homophones for deal: ['deal', 'dele']\n",
      "Homophones for levy: ['levy', 'levee']\n",
      "Homophones for so: ['so', 'sow', 'sew']\n",
      "Homophones for fair: ['fair', 'fare']\n",
      "Homophones for radical: ['radical', 'radicle']\n",
      "Homophones for take: []\n",
      "Homophones for host: []\n",
      "Homophones for four: ['four', 'for', 'fore']\n",
      "Homophones for day: []\n",
      "Homophones for together: []\n",
      "----600/14899----\n",
      "Homophones for world: ['world', 'whirled']\n",
      "Homophones for date: []\n",
      "Homophones for ten: []\n",
      "Homophones for silence: ['silence', 'silents']\n",
      "Homophones for few: ['few', 'phew']\n",
      "Homophones for still: []\n",
      "Homophones for light: ['light', 'lite']\n",
      "Homophones for pop: []\n",
      "Homophones for scene: ['scene', 'seen']\n",
      "Homophones for real: ['real', 'reel']\n",
      "Homophones for mark: ['mark', 'marc', 'marque']\n",
      "Homophones for form: ['form', 'forme']\n",
      "Homophones for fine: []\n",
      "Homophones for through: ['through', 'threw', 'thru']\n",
      "Homophones for presents: ['presents', 'presence']\n",
      "Homophones for each: []\n",
      "Homophones for etc: []\n",
      "Homophones for style: ['style', 'stile']\n",
      "Homophones for rock: ['rock', 'roc']\n",
      "Homophones for like: []\n",
      "Homophones for man: []\n",
      "Homophones for born: ['born', 'bourn', 'bourne']\n",
      "Homophones for net: []\n",
      "----700/14899----\n",
      "Homophones for way: ['way', 'weigh', 'whey']\n",
      "Homophones for seems: ['seems', 'seams']\n",
      "Homophones for feet: ['feet', 'feat']\n",
      "Homophones for once: []\n",
      "Homophones for come: ['come', 'cum']\n",
      "Homophones for leave: ['leave', 'lieve']\n",
      "Homophones for sane: []\n",
      "Homophones for therefore: ['therefore', 'therefor']\n",
      "Homophones for mask: ['mask', 'masque']\n",
      "Homophones for au: []\n",
      "----800/14899----\n",
      "Homophones for mr: []\n",
      "Homophones for prior: ['prior', 'prier']\n",
      "Homophones for plan: []\n",
      "Homophones for care: []\n",
      "Homophones for none: ['none', 'nun']\n",
      "Homophones for ready: []\n",
      "Homophones for save: []\n",
      "Homophones for rule: []\n",
      "Homophones for reach: []\n",
      "Homophones for la: []\n",
      "Homophones for presse: []\n",
      "Homophones for lance: ['lance', 'launce']\n",
      "Homophones for i: []\n",
      "Homophones for told: ['told', 'tolled']\n",
      "Homophones for stay: []\n",
      "Homophones for adds: ['adds', 'ads', 'adze']\n",
      "Homophones for current: ['current', 'currant']\n",
      "----900/14899----\n",
      "Homophones for overseas: ['overseas', 'oversees']\n",
      "Homophones for end: []\n",
      "Homophones for nine: []\n",
      "Homophones for wild: ['wild', 'whiled']\n",
      "Homophones for site: ['site', 'cite', 'sight']\n",
      "Homophones for via: []\n",
      "Homophones for press: []\n",
      "Homophones for adventures: []\n",
      "Homophones for cat: []\n",
      "Homophones for she: ['she', 'sidhe']\n",
      "Homophones for law: []\n",
      "Homophones for order: []\n",
      "----1000/14899----\n",
      "Homophones for ever: []\n",
      "Homophones for sense: ['sense', 'cense', 'cents', 'scents']\n",
      "Homophones for week: ['week', 'weak']\n",
      "Homophones for us: []\n",
      "Homophones for fail: []\n",
      "Homophones for mp: []\n",
      "Homophones for met: []\n",
      "Homophones for him: ['him', 'hymn']\n",
      "Homophones for my: []\n",
      "----1100/14899----\n",
      "Homophones for oversees: ['oversees', 'overseas']\n",
      "Homophones for hear: ['hear', 'here']\n",
      "Homophones for home: ['home', 'holm', 'hom']\n",
      "Homophones for act: []\n",
      "Homophones for well: []\n",
      "Homophones for offer: []\n",
      "Homophones for may: []\n",
      "Homophones for bell: ['bell', 'belle', 'bel']\n",
      "Homophones for does: ['does', 'dos', 'doughs', 'doze']\n",
      "Homophones for resist: []\n",
      "----1200/14899----\n",
      "Homophones for cite: ['cite', 'sight', 'site']\n",
      "Homophones for made: ['made', 'maid']\n",
      "Homophones for throne: ['throne', 'thrown']\n",
      "Homophones for rise: ['rise', 'ryes', 'ryse']\n",
      "Homophones for profit: ['profit', 'prophet']\n",
      "Homophones for cad: []\n",
      "Homophones for fourth: ['fourth', 'forth']\n",
      "Homophones for due: ['due', 'dew', 'do']\n",
      "Homophones for peer: ['peer', 'pier']\n",
      "----1300/14899----\n",
      "Homophones for principal: ['principal', 'principle']\n",
      "Homophones for rose: ['rose', 'roes', 'rows']\n",
      "Homophones for me: ['me', 'mi']\n",
      "Homophones for horse: ['horse', 'hoarse']\n",
      "Homophones for meat: ['meat', 'meet', 'mete']\n",
      "Homophones for sold: ['sold', 'soled']\n",
      "Homophones for list: []\n",
      "Homophones for present: []\n",
      "Homophones for presence: ['presence', 'presents']\n",
      "Homophones for route: ['route', 'root']\n",
      "Homophones for morning: ['morning', 'mourning']\n",
      "Homophones for store: []\n",
      "Homophones for board: ['board', 'bored']\n",
      "----1400/14899----\n",
      "Homophones for whole: ['whole', 'hole']\n",
      "Homophones for bin: ['bin', 'bine', 'been']\n",
      "Homophones for seat: ['seat', 'cete']\n",
      "Homophones for paid: ['paid', 'payed']\n",
      "Homophones for wait: ['wait', 'weight']\n",
      "Homophones for they're: [\"they're\", 'their', 'there']\n",
      "Homophones for gate: ['gate', 'gait']\n",
      "Homophones for run: []\n",
      "Homophones for it's: [\"it's\", 'its']\n",
      "Homophones for won't: [\"won't\", 'wont']\n",
      "Homophones for line: []\n",
      "Homophones for owe: ['owe', 'oh']\n",
      "Homophones for sure: []\n",
      "Homophones for know: ['know', 'no']\n",
      "Homophones for summer: []\n",
      "Homophones for set: []\n",
      "Homophones for view: []\n",
      "Homophones for sign: ['sign', 'sine', 'syne']\n",
      "Homophones for air: ['air', 'aire', 'are', 'ayre', 'ere', 'err', 'eyre', 'heir']\n",
      "Homophones for inc: []\n",
      "Homophones for fellow: ['fellow', 'felloe']\n",
      "Homophones for later: []\n",
      "Homophones for jay: ['jay', 'j']\n",
      "Homophones for he'd: [\"he'd\", 'heed']\n",
      "Homophones for seen: ['seen', 'scene']\n",
      "----1500/14899----\n",
      "Homophones for breaking: ['breaking', 'braking']\n",
      "Homophones for plane: ['plane', 'plain']\n",
      "Homophones for away: ['away', 'aweigh']\n",
      "Homophones for steer: ['steer', 'stere']\n",
      "Homophones for flier: ['flier', 'flyer']\n",
      "Homophones for earn: ['earn', 'erne', 'urn']\n",
      "Homophones for base: ['base', 'bass']\n",
      "Homophones for 9: []\n",
      "Homophones for late: []\n",
      "Homophones for hours: ['hours', 'ours']\n",
      "Homophones for least: ['least', 'leased']\n",
      "Homophones for six: ['six', 'sics']\n",
      "Homophones for news: ['news', 'gnus']\n",
      "----1600/14899----\n",
      "Homophones for stolen: ['stolen', 'stollen']\n",
      "Homophones for incident: []\n",
      "Homophones for existence: []\n",
      "Homophones for needs: ['needs', 'kneads']\n",
      "Homophones for great: ['great', 'grate']\n",
      "Homophones for tens: []\n",
      "----1700/14899----\n",
      "Homophones for chair: ['chair', 'chare']\n",
      "Homophones for zero: ['zero', 'xero-']\n",
      "Homophones for titan: ['titan', 'tighten']\n",
      "Homophones for close: ['close', 'clothes']\n",
      "Homophones for tire: ['tire', 'tyer', 'tyre']\n",
      "Homophones for hard: []\n",
      "Homophones for please: ['please', 'pleas']\n",
      "Homophones for sorry: ['sorry', 'sari']\n",
      "Homophones for eight: ['eight', 'ait', 'ate']\n",
      "Homophones for room: []\n",
      "Homophones for why: ['why', 'wye', 'y']\n",
      "Homophones for right: ['right', 'wright', 'rite', 'write']\n",
      "----1800/14899----\n",
      "Homophones for let's: [\"let's\", 'lets']\n",
      "Homophones for knows: ['knows', 'noes', 'nose']\n",
      "Homophones for sit: []\n",
      "Homophones for step: ['step', 'steppe']\n",
      "Homophones for seal: ['seal', 'ceil', 'seel']\n",
      "Homophones for guarantee: ['guarantee', 'guaranty']\n",
      "Homophones for accept: []\n",
      "Homophones for phase: ['phase', 'fays', 'faze']\n",
      "Homophones for fell: []\n",
      "Homophones for sent: ['sent', 'cent', 'scent']\n",
      "Homophones for analyst: ['analyst', 'annalist']\n",
      "Homophones for lowered: []\n",
      "Homophones for low: ['low', 'lo']\n",
      "Homophones for far: []\n",
      "----1900/14899----\n",
      "Homophones for done: ['done', 'dun']\n",
      "Homophones for planes: ['planes', 'plains']\n",
      "Homophones for eis: []\n",
      "Homophones for slow: ['slow', 'sloe']\n",
      "Homophones for pace: []\n",
      "Homophones for sa: []\n",
      "----2000/14899----\n",
      "Homophones for dent: []\n",
      "Homophones for de: []\n",
      "Homophones for men: []\n",
      "Homophones for off: []\n",
      "Homophones for cross: ['cross', 'crosse']\n",
      "Homophones for des: []\n",
      "Homophones for police: ['police', 'pelisse']\n",
      "Homophones for hand: []\n",
      "Homophones for teams: ['teams', 'teems']\n",
      "Homophones for local: []\n",
      "Homophones for le: []\n",
      "Homophones for steps: ['steps', 'steppes']\n",
      "Homophones for assistance: ['assistance', 'assistants']\n",
      "Homophones for post: []\n",
      "Homophones for bodies: ['bodies', \"body's\"]\n",
      "Homophones for asks: []\n",
      "----2100/14899----\n",
      "Homophones for blue: ['blue', 'blew']\n",
      "Homophones for bock: []\n",
      "Homophones for her: []\n",
      "Homophones for better: ['better', 'bettor']\n",
      "Homophones for wasted: ['wasted', 'waisted']\n",
      "Homophones for place: ['place', 'plaice']\n",
      "Homophones for main: ['main', 'mane']\n",
      "----2200/14899----\n",
      "Homophones for father: ['father', 'fother']\n",
      "Homophones for passed: ['passed', 'past']\n",
      "Homophones for tense: ['tense', 'tents']\n",
      "Homophones for ones: []\n",
      "Homophones for signs: ['signs', 'sines']\n",
      "Homophones for night: ['night', 'knight']\n",
      "Homophones for body: []\n",
      "Homophones for bed: []\n",
      "Homophones for cold: []\n",
      "Homophones for faint: ['faint', 'feint']\n",
      "Homophones for came: ['came', 'kame']\n",
      "Homophones for mind: ['mind', 'mined']\n",
      "Homophones for mourning: ['mourning', 'morning']\n",
      "Homophones for dying: ['dying', 'dyeing']\n",
      "Homophones for complete: ['complete', 'compleat']\n",
      "Homophones for am: []\n",
      "Homophones for old: []\n",
      "Homophones for ill: []\n",
      "Homophones for died: ['died', 'dyed']\n",
      "Homophones for stayed: ['stayed', 'stade', 'staid']\n",
      "Homophones for hands: []\n",
      "Homophones for mine: []\n",
      "Homophones for lent: []\n",
      "Homophones for lead: ['lead', 'led']\n",
      "Homophones for allowed: ['allowed', 'aloud']\n",
      "----2300/14899----\n",
      "Homophones for die: ['die', 'dye']\n",
      "Homophones for here: ['here', 'hear']\n",
      "Homophones for feel: ['feel', 'feal']\n",
      "Homophones for absence: ['absence', 'absents']\n",
      "Homophones for boy: ['boy', 'buoy']\n",
      "Homophones for tell: ['tell', 'tel']\n",
      "Homophones for yes: []\n",
      "Homophones for east: []\n",
      "Homophones for independent: []\n",
      "Homophones for try: ['try', 'tri']\n",
      "Homophones for eyes: ['eyes', 'aes', 'ayes']\n",
      "Homophones for resting: []\n",
      "Homophones for anger: []\n",
      "Homophones for eased: []\n",
      "Homophones for pain: ['pain', 'pane']\n",
      "----2400/14899----\n",
      "Homophones for silent: []\n",
      "Homophones for rings: ['rings', 'wrings']\n",
      "Homophones for days: ['days', 'daze']\n",
      "Homophones for bad: ['bad', 'bade']\n",
      "Homophones for course: ['course', 'coarse', 'corse']\n",
      "Homophones for hurts: ['hurts', 'hertz']\n",
      "Homophones for tear: ['tear', 'tare']\n",
      "Homophones for heart: ['heart', 'hart']\n",
      "Homophones for attend: []\n",
      "Homophones for birth: ['birth', 'berth']\n",
      "Homophones for lose: ['lose', 'loos']\n",
      "Homophones for ask: []\n",
      "Homophones for bit: ['bit', 'bitt']\n",
      "Homophones for alter: ['alter', 'altar']\n",
      "Homophones for ego: []\n",
      "Homophones for son: ['son', 'sun', 'sunn']\n",
      "Homophones for alone: []\n",
      "----2500/14899----\n",
      "Homophones for sing: []\n",
      "Homophones for soul: ['soul', 'sol', 'sole']\n",
      "Homophones for prime: []\n",
      "Homophones for aid: ['aid', 'aide', 'ade']\n",
      "Homophones for side: ['side', 'sighed']\n",
      "Homophones for peace: ['peace', 'piece']\n",
      "Homophones for court: []\n",
      "Homophones for top: []\n",
      "----2600/14899----\n",
      "Homophones for hair: ['hair', 'hare']\n",
      "Homophones for heroin: ['heroin', 'heroine']\n",
      "Homophones for racket: ['racket', 'rackett', 'racquet']\n",
      "Homophones for cent: ['cent', 'scent', 'sent']\n",
      "Homophones for heard: ['heard', 'herd']\n",
      "Homophones for ran: []\n",
      "Homophones for key: ['key', 'quay']\n",
      "Homophones for knew: ['knew', 'gnu', 'new', 'nu']\n",
      "Homophones for anne: []\n",
      "Homophones for independence: ['independence', 'independents']\n",
      "----2700/14899----\n",
      "Homophones for instil: []\n",
      "Homophones for hide: ['hide', 'hied']\n",
      "Homophones for let: []\n",
      "Homophones for session: ['session', 'cession']\n",
      "Homophones for row: ['row', 'rho', 'roe']\n",
      "Homophones for guest: ['guest', 'guessed']\n",
      "Homophones for ali: []\n",
      "Homophones for queen: ['queen', 'quean']\n",
      "Homophones for hall: ['hall', 'haul']\n",
      "Homophones for mrs: []\n",
      "Homophones for don: []\n",
      "Homophones for star: []\n",
      "Homophones for mantle: ['mantle', 'mantel']\n",
      "Homophones for los: []\n",
      "----2800/14899----\n",
      "Homophones for wave: ['wave', 'waive']\n",
      "Homophones for staff: ['staff', 'staph']\n",
      "Homophones for chance: ['chance', 'chants']\n",
      "Homophones for show: []\n",
      "Homophones for du: []\n",
      "Homophones for ear: []\n",
      "Homophones for tune: ['tune', 'toon']\n",
      "----2900/14899----\n",
      "Homophones for intent: []\n",
      "Homophones for bert: []\n",
      "Homophones for holy: ['holy', 'holey', 'wholly']\n",
      "Homophones for j: ['j', 'jay']\n",
      "Homophones for altar: ['altar', 'alter']\n",
      "Homophones for add: ['add', 'ad']\n",
      "Homophones for cited: ['cited', 'sighted', 'sited']\n",
      "Homophones for acts: []\n",
      "----3000/14899----\n",
      "Homophones for settle: []\n",
      "Homophones for rue: ['rue', 'roo', 'roux']\n",
      "Homophones for tool: ['tool', 'tule', 'tulle']\n",
      "Homophones for hour: ['hour', 'our']\n",
      "Homophones for cause: ['cause', 'caws']\n",
      "Homophones for red: ['red', 'redd', 'read']\n",
      "Homophones for wish: ['wish', 'whish']\n",
      "Homophones for patients: ['patients', 'patience']\n",
      "Homophones for suit: ['suit', 'soot']\n",
      "----3100/14899----\n",
      "Homophones for mall: ['mall', 'maul']\n",
      "Homophones for ire: ['ire', 'eyer']\n",
      "Homophones for camera: ['camera', 'camara']\n",
      "Homophones for fined: ['fined', 'find']\n",
      "Homophones for barred: ['barred', 'bard']\n",
      "Homophones for seize: ['seize', \"c's\", 'cees', 'seas', 'sees']\n",
      "Homophones for wrote: ['wrote', 'rote']\n",
      "----3200/14899----\n",
      "Homophones for hold: ['hold', 'holed']\n",
      "Homophones for residence: []\n",
      "Homophones for fill: []\n",
      "Homophones for sex: ['sex', 'sects']\n",
      "Homophones for adolescence: ['adolescence', 'adolescents']\n",
      "Homophones for sister: []\n",
      "Homophones for martin: ['martin', 'marten']\n",
      "Homophones for merging: []\n",
      "----3300/14899----\n",
      "Homophones for check: ['check', 'cheque']\n",
      "Homophones for principle: ['principle', 'principal']\n",
      "Homophones for border: ['border', 'boarder']\n",
      "Homophones for raid: ['raid', 'rayed']\n",
      "Homophones for missiles: ['missiles', 'missals']\n",
      "Homophones for nor: []\n",
      "Homophones for respond: []\n",
      "Homophones for &: []\n",
      "----3400/14899----\n",
      "Homophones for s&p: []\n",
      "Homophones for eu: []\n",
      "Homophones for loan: ['loan', 'lone']\n",
      "Homophones for trade: ['trade', 'trayed']\n",
      "----3500/14899----\n",
      "Homophones for merge: []\n",
      "Homophones for bases: ['bases', 'basses']\n",
      "Homophones for colonel: ['colonel', 'kernel']\n",
      "Homophones for hill: []\n",
      "Homophones for peak: ['peak', 'peek', 'peke', 'pique']\n",
      "Homophones for insurgents: ['insurgents', 'insurgence']\n",
      "Homophones for war: ['war', 'wore']\n",
      "Homophones for rate: []\n",
      "----3600/14899----\n",
      "Homophones for raised: ['raised', 'razed']\n",
      "Homophones for seemed: ['seemed', 'seamed']\n",
      "Homophones for patient: []\n",
      "----3700/14899----\n",
      "Homophones for consult: []\n",
      "Homophones for permanent: []\n",
      "Homophones for ars: []\n",
      "Homophones for lied: ['lied', 'lead', 'lede']\n",
      "Homophones for marks: ['marks', 'marcs', 'marques']\n",
      "----3800/14899----\n",
      "Homophones for greater: ['greater', 'grater']\n",
      "Homophones for team: ['team', 'teem']\n",
      "Homophones for councillor: ['councillor', 'councilor', 'counsellor', 'counselor']\n",
      "Homophones for ann: []\n",
      "Homophones for un: []\n",
      "Homophones for role: ['role', 'roll']\n",
      "Homophones for rio: []\n",
      "Homophones for draft: ['draft', 'draught']\n",
      "Homophones for ms: []\n",
      "Homophones for ban: ['ban', 'bann']\n",
      "Homophones for chant: []\n",
      "----3900/14899----\n",
      "Homophones for worst: ['worst', 'wurst']\n",
      "Homophones for stakes: ['stakes', 'steaks']\n",
      "Homophones for coal: ['coal', 'cole', 'kohl']\n",
      "Homophones for vale: ['vale', 'vail', 'veil']\n",
      "Homophones for eni: []\n",
      "Homophones for kill: ['kill', 'kiln']\n",
      "Homophones for insist: ['insist', 'encyst']\n",
      "Homophones for grown: ['grown', 'groan']\n",
      "Homophones for ft: []\n",
      "Homophones for ease: ['ease', \"e's\"]\n",
      "Homophones for win: ['win', 'whin', 'wynn']\n",
      "----4000/14899----\n",
      "Homophones for miners: ['miners', 'minors']\n",
      "Homophones for coy: ['coy', 'koi']\n",
      "Homophones for warn: ['warn', 'worn']\n",
      "Homophones for lima: []\n",
      "Homophones for taught: ['taught', 'taut']\n",
      "Homophones for tales: ['tales', 'tails']\n",
      "Homophones for brute: ['brute', 'bruit', 'brut']\n",
      "Homophones for subtle: []\n",
      "Homophones for writing: ['writing', 'righting']\n",
      "Homophones for sight: ['sight', 'cite', 'site']\n",
      "Homophones for adult: []\n",
      "Homophones for evil: []\n",
      "Homophones for story: ['story', 'storey']\n",
      "----4100/14899----\n",
      "Homophones for link: []\n",
      "Homophones for relayed: ['relayed', 'relaid']\n",
      "Homophones for citing: ['citing', 'sighting', 'siting']\n",
      "Homophones for innocent: []\n",
      "Homophones for manner: ['manner', 'manor']\n",
      "Homophones for grow: ['grow', 'gros']\n",
      "Homophones for meta: []\n",
      "----4200/14899----\n",
      "Homophones for knee: ['knee', 'nee']\n",
      "Homophones for lay: ['lay', 'lei', 'ley']\n",
      "Homophones for click: ['click', 'clique']\n",
      "Homophones for en: ['en', 'n']\n",
      "Homophones for ashes: []\n",
      "Homophones for fact: []\n",
      "----4300/14899----\n",
      "Homophones for rat: []\n",
      "Homophones for frank: ['frank', 'franc']\n",
      "Homophones for fay: ['fay', 'fey']\n",
      "----4400/14899----\n",
      "Homophones for nick: []\n",
      "Homophones for cast: ['cast', 'caste']\n",
      "Homophones for shoes: ['shoes', 'shoos']\n",
      "Homophones for wry: ['wry', 'rye']\n",
      "Homophones for w: []\n",
      "Homophones for meeting: ['meeting', 'meting']\n",
      "Homophones for takes: []\n",
      "Homophones for he's: []\n",
      "Homophones for he'll: [\"he'll\", 'heal', 'heel']\n",
      "Homophones for alley: []\n",
      "Homophones for hey: ['hey', 'hae', 'hay']\n",
      "Homophones for maybe: []\n",
      "Homophones for i'll: []\n",
      "Homophones for crude: ['crude', 'crewed']\n",
      "Homophones for weighed: ['weighed', 'wade']\n",
      "----4500/14899----\n",
      "Homophones for ice: []\n",
      "Homophones for status: ['status', 'statice']\n",
      "Homophones for bas: []\n",
      "Homophones for seem: ['seem', 'seam']\n",
      "Homophones for threw: ['threw', 'through', 'thru']\n",
      "Homophones for wide: ['wide', \"why'd\"]\n",
      "Homophones for mix: ['mix', 'micks']\n",
      "----4600/14899----\n",
      "Homophones for needed: ['needed', 'kneaded']\n",
      "Homophones for rights: ['rights', 'wrights', 'rites', 'writes']\n",
      "Homophones for oversee: []\n",
      "Homophones for oral: ['oral', 'aural']\n",
      "Homophones for lower: []\n",
      "Homophones for waste: ['waste', 'waist']\n",
      "Homophones for rent: []\n",
      "Homophones for assistant: []\n",
      "Homophones for emerged: ['emerged', 'immerged']\n",
      "Homophones for competence: ['competence', 'competents']\n",
      "Homophones for confectionery: ['confectionery', 'confectionary']\n",
      "----4700/14899----\n",
      "Homophones for enter: []\n",
      "Homophones for palate: ['palate', 'palette', 'pallet']\n",
      "Homophones for eat: []\n",
      "Homophones for rest: ['rest', 'wrest']\n",
      "Homophones for praise: ['praise', 'prase', 'prays', 'preys']\n",
      "Homophones for harm: []\n",
      "Homophones for andy: []\n",
      "Homophones for bailey: ['bailey', 'bailee', 'bailie']\n",
      "----4800/14899----\n",
      "Homophones for oh: ['oh', 'owe']\n",
      "Homophones for she's: [\"she's\", 'shes']\n",
      "Homophones for tape: []\n",
      "Homophones for we're: [\"we're\", 'weir']\n",
      "Homophones for hack: []\n",
      "Homophones for al: []\n",
      "Homophones for : []\n",
      "Homophones for higher: ['higher', 'hire']\n",
      "Homophones for grip: ['grip', 'grippe']\n",
      "----4900/14899----\n",
      "Homophones for bomb: ['bomb', 'balm']\n",
      "Homophones for symbol: ['symbol', 'cymbal']\n",
      "Homophones for petrol: ['petrol', 'petrel']\n",
      "Homophones for poll: ['poll', 'pole']\n",
      "Homophones for gay: []\n",
      "Homophones for bonds: ['bonds', 'bonze']\n",
      "Homophones for pope: []\n",
      "----5000/14899----\n",
      "Homophones for resident: []\n",
      "Homophones for combing: ['combing', 'coaming']\n",
      "Homophones for dance: []\n",
      "Homophones for belle: ['belle', 'bel', 'bell']\n",
      "Homophones for et: []\n",
      "----5100/14899----\n",
      "Homophones for les: []\n",
      "Homophones for matt: ['matt', 'matte', 'mat']\n",
      "Homophones for age: []\n",
      "Homophones for roles: ['roles', 'rolls']\n",
      "Homophones for poor: []\n",
      "Homophones for adolescents: ['adolescents', 'adolescence']\n",
      "Homophones for accident: []\n",
      "Homophones for ring: ['ring', 'wring']\n",
      "Homophones for jam: ['jam', 'jamb']\n",
      "Homophones for ride: []\n",
      "Homophones for till: ['till', 'til']\n",
      "----5200/14899----\n",
      "Homophones for charred: ['charred', 'chard']\n",
      "Homophones for lies: ['lies', 'lyes', 'lyse']\n",
      "Homophones for steal: ['steal', 'steel', 'stele']\n",
      "Homophones for word: ['word', 'whirred']\n",
      "Homophones for mutual: ['mutual', 'mutuel']\n",
      "Homophones for ey: []\n",
      "Homophones for lacks: ['lacks', 'lax']\n",
      "Homophones for dependence: ['dependence', 'dependents']\n",
      "----5300/14899----\n",
      "Homophones for thrown: ['thrown', 'throne']\n",
      "Homophones for lack: ['lack', 'lac']\n",
      "Homophones for tier: ['tier', 'tear']\n",
      "Homophones for mat: ['mat', 'matt', 'matte']\n",
      "Homophones for ga: []\n",
      "Homophones for freak: ['freak', 'phreak']\n",
      "Homophones for facts: ['facts', 'Fax']\n",
      "----5400/14899----\n",
      "Homophones for warrant: []\n",
      "Homophones for guide: ['guide', 'guyed']\n",
      "Homophones for tongue: ['tongue', 'tung']\n",
      "Homophones for rumor: ['rumor', 'roomer']\n",
      "Homophones for scale: []\n",
      "Homophones for read: ['read', 'rede', 'reed']\n",
      "Homophones for correspondence: ['correspondence', 'correspondents']\n",
      "----5500/14899----\n",
      "Homophones for pick: ['pick', 'pic']\n",
      "Homophones for male: ['male', 'mail']\n",
      "Homophones for dr: []\n",
      "Homophones for muscle: ['muscle', 'mussel']\n",
      "Homophones for mass: []\n",
      "Homophones for m: ['m', 'em']\n",
      "Homophones for f: []\n",
      "Homophones for x: ['x', 'ex']\n",
      "----5600/14899----\n",
      "Homophones for handsome: ['handsome', 'hansom']\n",
      "Homophones for dual: ['dual', 'duel']\n",
      "Homophones for forward: ['forward', 'foreward', 'foreword']\n",
      "Homophones for cues: ['cues', 'queues']\n",
      "Homophones for genes: ['genes', 'jeans']\n",
      "Homophones for advent: []\n",
      "----5700/14899----\n",
      "Homophones for androgen: []\n",
      "Homophones for write: ['write', 'right', 'rite', 'wright']\n",
      "Homophones for ok: []\n",
      "Homophones for fun: []\n",
      "Homophones for council: ['council', 'counsel']\n",
      "Homophones for fate: ['fate', 'fete']\n",
      "Homophones for polling: ['polling', 'poling']\n",
      "Homophones for gauge: ['gauge', 'gage']\n",
      "Homophones for correspondent: []\n",
      "----5800/14899----\n",
      "Homophones for fluorescent: []\n",
      "Homophones for reading: ['reading', 'reeding']\n",
      "Homophones for protein: ['protein', 'protean']\n",
      "Homophones for ph: []\n",
      "Homophones for chris: []\n",
      "Homophones for lick: []\n",
      "Homophones for beer: ['beer', 'bier']\n",
      "Homophones for leaf: ['leaf', 'lief']\n",
      "Homophones for lou: []\n",
      "Homophones for reed: ['reed', 'read', 'rede']\n",
      "Homophones for dies: ['dies', 'dyes']\n",
      "Homophones for tai: []\n",
      "Homophones for chi: []\n",
      "----5900/14899----\n",
      "Homophones for martial: ['martial', 'marshal']\n",
      "Homophones for art: []\n",
      "Homophones for prince: ['prince', 'prints']\n",
      "Homophones for bet: []\n",
      "Homophones for guess: []\n",
      "Homophones for there's: [\"there's\", 'theirs']\n",
      "Homophones for pet: []\n",
      "Homophones for guy: []\n",
      "Homophones for nice: ['nice', 'gneiss']\n",
      "----6000/14899----\n",
      "Homophones for we've: [\"we've\", 'weave']\n",
      "Homophones for you're: [\"you're\", 'yore', 'your']\n",
      "Homophones for tim: []\n",
      "Homophones for ross: []\n",
      "Homophones for rally: []\n",
      "Homophones for seats: []\n",
      "Homophones for adventure: []\n",
      "Homophones for uk: []\n",
      "Homophones for entrants: ['entrants', 'entrance']\n",
      "Homophones for compete: []\n",
      "Homophones for desert: ['desert', 'dessert']\n",
      "Homophones for crews: ['crews', 'cruise']\n",
      "Homophones for ledge: []\n",
      "Homophones for minor: ['minor', 'miner']\n",
      "----6100/14899----\n",
      "Homophones for sites: ['sites', 'cites', 'sights']\n",
      "Homophones for station: []\n",
      "Homophones for lau: []\n",
      "Homophones for miss: []\n",
      "Homophones for mick: []\n",
      "Homophones for pass: []\n",
      "Homophones for entrance: ['entrance', 'entrants']\n",
      "Homophones for cue: ['cue', 'q', 'queue']\n",
      "----6200/14899----\n",
      "Homophones for cheap: ['cheap', 'cheep']\n",
      "Homophones for tale: ['tale', 'tael', 'tail']\n",
      "Homophones for yoke: ['yoke', 'yolk']\n",
      "Homophones for equivalent: []\n",
      "Homophones for adherence: ['adherence', 'adherents']\n",
      "Homophones for won: ['won', 'one']\n",
      "Homophones for pen: []\n",
      "----6300/14899----\n",
      "Homophones for bloc: ['bloc', 'block']\n",
      "Homophones for here's: [\"here's\", 'hears']\n",
      "Homophones for loose: ['loose', 'luce']\n",
      "Homophones for cosy: ['cosy', 'cosey', 'cozy']\n",
      "Homophones for dem: []\n",
      "Homophones for older: []\n",
      "Homophones for tend: []\n",
      "Homophones for mean: ['mean', 'mesne', 'mien']\n",
      "Homophones for imminent: ['imminent', 'immanent']\n",
      "Homophones for pat: []\n",
      "Homophones for slight: ['slight', 'sleight']\n",
      "Homophones for stall: []\n",
      "Homophones for ward: ['ward', 'warred']\n",
      "----6400/14899----\n",
      "Homophones for weight: ['weight', 'wait']\n",
      "Homophones for links: ['links', 'lynx']\n",
      "Homophones for fat: ['fat', 'phat']\n",
      "Homophones for review: ['review', 'revue']\n",
      "Homophones for hire: ['hire', 'higher']\n",
      "Homophones for mary: []\n",
      "----6500/14899----\n",
      "Homophones for clothes: ['clothes', 'close']\n",
      "Homophones for coarse: ['coarse', 'corse', 'course']\n",
      "Homophones for who's: [\"who's\", 'whose']\n",
      "Homophones for van: []\n",
      "Homophones for magnificent: []\n",
      "Homophones for sas: []\n",
      "Homophones for arc: ['arc', 'ark']\n",
      "Homophones for eye: ['eye', 'ai', 'aye', 'I']\n",
      "----6600/14899----\n",
      "Homophones for bearing: ['bearing', 'baring']\n",
      "Homophones for pistol: ['pistol', 'pistil']\n",
      "Homophones for torture: ['torture', 'torcher']\n",
      "Homophones for writes: ['writes', 'rights', 'rites', 'wrights']\n",
      "Homophones for send: ['send', 'scend']\n",
      "Homophones for seer: ['seer', 'cere', 'sear', 'sere']\n",
      "Homophones for wheel: ['wheel', \"we'll\", 'weal', 'wheal']\n",
      "Homophones for hang: []\n",
      "Homophones for stake: ['stake', 'steak']\n",
      "----6700/14899----\n",
      "Homophones for co: []\n",
      "Homophones for vie: []\n",
      "Homophones for ad: ['ad', 'add']\n",
      "Homophones for sales: ['sales', 'sails']\n",
      "Homophones for waist: ['waist', 'waste']\n",
      "Homophones for fares: ['fares', 'fairs']\n",
      "Homophones for flyers: ['flyers', 'fliers']\n",
      "Homophones for depend: []\n",
      "----6800/14899----\n",
      "Homophones for aisle: ['aisle', \"I'll\", 'isle']\n",
      "Homophones for fly: []\n",
      "Homophones for oracle: ['oracle', 'auricle']\n",
      "Homophones for s&p-500: []\n",
      "Homophones for mice: []\n",
      "Homophones for prof: []\n",
      "----6900/14899----\n",
      "Homophones for e: []\n",
      "Homophones for quay: ['quay', 'cay', 'key']\n",
      "Homophones for lease: []\n",
      "Homophones for door: ['door', 'dor']\n",
      "Homophones for loud: []\n",
      "----7000/14899----\n",
      "Homophones for cousin: ['cousin', 'cozen']\n",
      "Homophones for rail: ['rail', 'rale']\n",
      "Homophones for baroness: ['baroness', 'barrenness']\n",
      "Homophones for complementing: ['complementing', 'complimenting']\n",
      "Homophones for phases: ['phases', 'fazes']\n",
      "Homophones for mode: ['mode', 'mowed']\n",
      "Homophones for crew: ['crew', 'krewe']\n",
      "----7100/14899----\n",
      "Homophones for height: ['height', 'hight']\n",
      "Homophones for belligerents: ['belligerents', 'belligerence']\n",
      "Homophones for pair: ['pair', 'pare', 'pear', 'pere']\n",
      "Homophones for lender: []\n",
      "Homophones for gross: []\n",
      "Homophones for ching: []\n",
      "----7200/14899----\n",
      "Homophones for ventures: []\n",
      "Homophones for tide: ['tide', 'tied']\n",
      "Homophones for flow: ['flow', 'floe']\n",
      "Homophones for chased: ['chased', 'chaste']\n",
      "Homophones for ss: []\n",
      "Homophones for tough: ['tough', 'tuff']\n",
      "Homophones for urgent: []\n",
      "----7300/14899----\n",
      "Homophones for mustard: ['mustard', 'mustered']\n",
      "Homophones for toll: ['toll', 'tole']\n",
      "Homophones for flee: ['flee', 'flea']\n",
      "Homophones for tight: []\n",
      "Homophones for rid: []\n",
      "----7400/14899----\n",
      "Homophones for pits: []\n",
      "Homophones for sect: []\n",
      "Homophones for jail: ['jail', 'gaol']\n",
      "Homophones for parole: ['parole', 'parol']\n",
      "Homophones for murder: []\n",
      "Homophones for eve: []\n",
      "Homophones for guys: ['guys', 'guise']\n",
      "Homophones for sari: ['sari', 'sorry']\n",
      "Homophones for bar: ['bar', 'barre']\n",
      "Homophones for cell: ['cell', 'cel', 'sell']\n",
      "----7500/14899----\n",
      "Homophones for attendants: ['attendants', 'attendance']\n",
      "Homophones for surf: ['surf', 'serf']\n",
      "Homophones for sen: []\n",
      "Homophones for resistant: []\n",
      "Homophones for emit: []\n",
      "----7600/14899----\n",
      "Homophones for dow: []\n",
      "Homophones for bird: ['bird', 'burd', 'burred']\n",
      "Homophones for sea: ['sea', 'c', 'cee', 'see']\n",
      "Homophones for rig: []\n",
      "Homophones for fit: []\n",
      "Homophones for polled: ['polled', 'poled']\n",
      "Homophones for views: []\n",
      "Homophones for sweets: ['sweets', 'suites']\n",
      "Homophones for sandy: ['sandy', 'sandhi']\n",
      "Homophones for wear: ['wear', 'ware', 'weir', 'where']\n",
      "Homophones for cool: []\n",
      "Homophones for era: []\n",
      "Homophones for continent: []\n",
      "Homophones for routes: ['routes', 'roots']\n",
      "----7700/14899----\n",
      "Homophones for cubic: []\n",
      "Homophones for aerial: ['aerial', 'ariel']\n",
      "Homophones for eez: []\n",
      "Homophones for xi: ['xi', 'psi', 'sigh']\n",
      "Homophones for lesser: ['lesser', 'lessor']\n",
      "Homophones for bo: []\n",
      "Homophones for sights: ['sights', 'cites', 'sites']\n",
      "Homophones for hears: ['hears', \"here's\"]\n",
      "----7800/14899----\n",
      "Homophones for hare: ['hare', 'hair']\n",
      "Homophones for counsel: ['counsel', 'council']\n",
      "Homophones for lap: []\n",
      "Homophones for lesson: ['lesson', 'lessen']\n",
      "Homophones for blair: ['blair', 'blare']\n",
      "Homophones for aired: ['aired', 'erred']\n",
      "Homophones for envoy: ['envoy', 'envoi']\n",
      "Homophones for prize: ['prize', 'pries', 'prise']\n",
      "Homophones for cap: []\n",
      "----7900/14899----\n",
      "Homophones for aunt: ['aunt', 'ant']\n",
      "Homophones for sand: []\n",
      "Homophones for mad: []\n",
      "Homophones for plea: []\n",
      "Homophones for chen: []\n",
      "Homophones for leak: ['leak', 'leek']\n",
      "Homophones for naval: ['naval', 'navel']\n",
      "Homophones for var: []\n",
      "----8000/14899----\n",
      "Homophones for purse: ['purse', 'perse']\n",
      "Homophones for lac: ['lac', 'lack']\n",
      "Homophones for tap: []\n",
      "Homophones for breach: ['breach', 'breech']\n",
      "Homophones for bail: ['bail', 'Baal', 'bale']\n",
      "----8100/14899----\n",
      "Homophones for invade: ['invade', 'inveighed']\n",
      "Homophones for precision: ['precision', 'precisian']\n",
      "Homophones for r&d: []\n",
      "Homophones for gait: ['gait', 'gate']\n",
      "Homophones for pack: []\n",
      "Homophones for file: ['file', 'faille', 'phial']\n",
      "----8200/14899----\n",
      "Homophones for pin: []\n",
      "Homophones for locks: ['locks', 'lochs', 'lox']\n",
      "Homophones for shed: []\n",
      "Homophones for tail: ['tail', 'tael', 'tale']\n",
      "Homophones for mood: ['mood', 'mooed']\n",
      "Homophones for tails: ['tails', 'tales']\n",
      "----8300/14899----\n",
      "Homophones for insight: ['insight', 'incite']\n",
      "Homophones for band: ['band', 'banned']\n",
      "Homophones for turner: []\n",
      "----8400/14899----\n",
      "Homophones for jean: ['jean', 'gene']\n",
      "Homophones for gop: []\n",
      "Homophones for p: ['p', 'pea', 'pee']\n",
      "Homophones for rude: ['rude', 'rood', 'rued']\n",
      "----8500/14899----\n",
      "Homophones for precede: []\n",
      "Homophones for mitt: []\n",
      "Homophones for lock: ['lock', 'lakh', 'loch', 'lough']\n",
      "Homophones for sank: ['sank', 'cinque']\n",
      "Homophones for magnate: ['magnate', 'magnet']\n",
      "Homophones for pencil: ['pencil', 'pencel', 'pensile']\n",
      "Homophones for reward: []\n",
      "Homophones for pour: ['pour', 'pore']\n",
      "----8600/14899----\n",
      "Homophones for whales: ['whales', 'wails']\n",
      "Homophones for mars: []\n",
      "Homophones for instant: []\n",
      "Homophones for citi: []\n",
      "----8700/14899----\n",
      "Homophones for cac: []\n",
      "Homophones for at&t: []\n",
      "Homophones for sole: ['sole', 'sol', 'soul']\n",
      "Homophones for bond: []\n",
      "Homophones for block: ['block', 'bloc']\n",
      "Homophones for flowers: ['flowers', 'flours']\n",
      "Homophones for plant: []\n",
      "Homophones for sami: []\n",
      "----8800/14899----\n",
      "Homophones for ngo: []\n",
      "Homophones for ink: ['ink', 'Inc.']\n",
      "Homophones for piece: ['piece', 'peace']\n",
      "Homophones for carol: ['carol', 'carrel']\n",
      "Homophones for sequence: ['sequence', 'sequents']\n",
      "Homophones for inks: []\n",
      "Homophones for grey: ['grey', 'gray']\n",
      "----8900/14899----\n",
      "Homophones for bow: ['bow', 'beau']\n",
      "Homophones for ham: []\n",
      "Homophones for sued: ['sued', 'pseud']\n",
      "Homophones for corp: []\n",
      "Homophones for core: ['core', 'corps', 'kor']\n",
      "Homophones for lit: []\n",
      "Homophones for chu: []\n",
      "----9000/14899----\n",
      "Homophones for oath: []\n",
      "Homophones for shown: ['shown', 'shone']\n",
      "Homophones for pomp: []\n",
      "Homophones for tyre: ['tyre', 'tire', 'tyer']\n",
      "Homophones for tie: ['tie', 'tye']\n",
      "Homophones for marshal: ['marshal', 'martial']\n",
      "Homophones for attendance: ['attendance', 'attendants']\n",
      "Homophones for chants: ['chants', 'chance']\n",
      "----9100/14899----\n",
      "Homophones for ac: []\n",
      "Homophones for foul: ['foul', 'fowl']\n",
      "Homophones for st: []\n",
      "Homophones for hurt: []\n",
      "----9200/14899----\n",
      "Homophones for weighting: ['weighting', 'waiting']\n",
      "Homophones for gun: []\n",
      "Homophones for minors: ['minors', 'miners']\n",
      "Homophones for resistance: ['resistance', 'resistants']\n",
      "Homophones for borough: ['borough', 'burro', 'burrow']\n",
      "----9300/14899----\n",
      "Homophones for pixie: ['pixie', 'pyxie']\n",
      "Homophones for babe: []\n",
      "Homophones for hero: []\n",
      "Homophones for arm: []\n",
      "Homophones for rough: ['rough', 'ruff']\n",
      "----9400/14899----\n",
      "Homophones for heir: ['heir', 'air', 'aire', 'are', 'ayre', 'ere', 'err', 'eyre']\n",
      "Homophones for flea: ['flea', 'flee']\n",
      "----9500/14899----\n",
      "Homophones for venture: []\n",
      "Homophones for curse: []\n",
      "Homophones for cum: ['cum', 'come']\n",
      "Homophones for dol: []\n",
      "Homophones for marc: ['marc', 'mark', 'marque']\n",
      "Homophones for boot: []\n",
      "----9600/14899----\n",
      "Homophones for we'd: [\"we'd\", 'weed']\n",
      "Homophones for wrapped: ['wrapped', 'rapped', 'rapt']\n",
      "Homophones for plain: ['plain', 'plane']\n",
      "Homophones for fake: []\n",
      "Homophones for liar: ['liar', 'lier', 'lyre']\n",
      "Homophones for traitor: []\n",
      "Homophones for size: ['size', 'scyes', 'scythes', 'sighs ']\n",
      "Homophones for kitten: []\n",
      "----9700/14899----\n",
      "Homophones for licked: ['licked', 'licht']\n",
      "Homophones for mucous: ['mucous', 'mucus']\n",
      "Homophones for phrase: ['phrase', 'fraise', 'frays']\n",
      "Homophones for mic: []\n",
      "Homophones for aide: ['aide', 'ade', 'aid']\n",
      "Homophones for seated: []\n",
      "Homophones for pressed: ['pressed', 'prest']\n",
      "Homophones for wore: ['wore', 'war']\n",
      "Homophones for fan: []\n",
      "Homophones for pot: []\n",
      "----9800/14899----\n",
      "Homophones for mail: ['mail', 'male']\n",
      "Homophones for stance: []\n",
      "Homophones for rancor: ['rancor', 'ranker']\n",
      "Homophones for assist: []\n",
      "Homophones for baht: ['baht', 'bot']\n",
      "----9900/14899----\n",
      "Homophones for roots: ['roots', 'routes']\n",
      "Homophones for lamb: ['lamb', 'lam']\n",
      "Homophones for sweet: ['sweet', 'suite']\n",
      "Homophones for pad: []\n",
      "Homophones for lightning: ['lightning', 'lightening']\n",
      "----10000/14899----\n",
      "Homophones for wit: ['wit', 'whit']\n",
      "Homophones for choir: ['choir', 'quire']\n",
      "Homophones for r: []\n",
      "Homophones for premiere: ['premiere', 'premier']\n",
      "Homophones for lea: ['lea', 'lee', 'li']\n",
      "----10100/14899----\n",
      "Homophones for pac: []\n",
      "Homophones for independents: ['independents', 'independence']\n",
      "Homophones for ball: ['ball', 'bawl']\n",
      "Homophones for ted: []\n",
      "Homophones for bare: ['bare', 'bair', 'bear']\n",
      "Homophones for und: []\n",
      "----10200/14899----\n",
      "Homophones for missed: ['missed', 'mist']\n",
      "----10300/14899----\n",
      "Homophones for riot: ['riot', 'ryot']\n",
      "Homophones for emerges: ['emerges', 'immerges']\n",
      "Homophones for caries: ['caries', 'carries']\n",
      "Homophones for dental: ['dental', 'dentil']\n",
      "Homophones for instance: ['instance', 'instants']\n",
      "----10400/14899----\n",
      "Homophones for anti: []\n",
      "Homophones for bold: ['bold', 'bowled']\n",
      "Homophones for adz: []\n",
      "Homophones for carrie: []\n",
      "Homophones for chuck: []\n",
      "Homophones for weekly: ['weekly', 'weakly']\n",
      "Homophones for tuc: []\n",
      "Homophones for tory: []\n",
      "----10500/14899----\n",
      "Homophones for altogether: []\n",
      "Homophones for bizarre: ['bizarre', 'bazaar']\n",
      "Homophones for app: []\n",
      "Homophones for ab: []\n",
      "----10600/14899----\n",
      "Homophones for beat: ['beat', 'beet']\n",
      "Homophones for ritz: []\n",
      "Homophones for eva: []\n",
      "Homophones for idol: ['idol', 'idle', 'idyll']\n",
      "Homophones for suite: ['suite', 'sweet']\n",
      "Homophones for ewa: []\n",
      "Homophones for bowl: ['bowl', 'bole', 'boll']\n",
      "Homophones for pours: ['pours', 'pores']\n",
      "Homophones for cashes: ['cashes', 'caches']\n",
      "Homophones for spit: []\n",
      "----10700/14899----\n",
      "Homophones for liquor: ['liquor', 'licker']\n",
      "Homophones for wa: []\n",
      "Homophones for ian: []\n",
      "Homophones for ks: []\n",
      "Homophones for stan: []\n",
      "Homophones for racquet: ['racquet', 'racket', 'rackett']\n",
      "Homophones for sam: []\n",
      "Homophones for berth: ['berth', 'birth']\n",
      "Homophones for fray: []\n",
      "----10800/14899----\n",
      "Homophones for pouring: ['pouring', 'poring']\n",
      "Homophones for tents: ['tents', 'tense']\n",
      "Homophones for razor: ['razor', 'raiser', 'razer']\n",
      "Homophones for metal: ['metal', 'mettle']\n",
      "Homophones for edge: []\n",
      "----10900/14899----\n",
      "Homophones for weighs: ['weighs', 'ways']\n",
      "Homophones for tied: ['tied', 'tide']\n",
      "Homophones for beach: ['beach', 'beech']\n",
      "Homophones for pi: ['pi', 'pie', 'pye']\n",
      "Homophones for psi: ['psi', 'sigh', 'xi']\n",
      "Homophones for rite: ['rite', 'write', 'right', 'wright']\n",
      "Homophones for waited: ['waited', 'weighted']\n",
      "Homophones for banned: ['banned', 'band']\n",
      "Homophones for hike: ['hike', 'haik']\n",
      "Homophones for bull: []\n",
      "Homophones for climb: ['climb', 'clime']\n",
      "----11000/14899----\n",
      "Homophones for lag: []\n",
      "Homophones for exist: []\n",
      "Homophones for tighter: ['tighter', 'titer']\n",
      "Homophones for fare: ['fare', 'fair']\n",
      "Homophones for weak: ['weak', 'week']\n",
      "Homophones for pure: ['pure', 'puer']\n",
      "Homophones for rip: []\n",
      "----11100/14899----\n",
      "Homophones for alan: []\n",
      "Homophones for toes: ['toes', 'tows']\n",
      "Homophones for bury: ['bury', 'berry']\n",
      "Homophones for packed: ['packed', 'pact']\n",
      "Homophones for ceiling: ['ceiling', 'sealing']\n",
      "Homophones for king: []\n",
      "Homophones for descend: []\n",
      "Homophones for rain: ['rain', 'reign', 'rein']\n",
      "Homophones for b&q: []\n",
      "----11200/14899----\n",
      "Homophones for sum: ['sum', 'some']\n",
      "Homophones for lets: ['lets', \"let's\"]\n",
      "Homophones for alto: []\n",
      "----11300/14899----\n",
      "Homophones for cannon: ['cannon', 'canon']\n",
      "Homophones for throw: ['throw', 'throe']\n",
      "Homophones for cet: []\n",
      "Homophones for cops: ['cops', 'copse']\n",
      "Homophones for sun: ['sun', 'sunn', 'son']\n",
      "Homophones for bailed: ['bailed', 'baled']\n",
      "----11400/14899----\n",
      "Homophones for finn: []\n",
      "Homophones for leg: []\n",
      "Homophones for erred: ['erred', 'aired']\n",
      "Homophones for summary: ['summary', 'summery']\n",
      "----11500/14899----\n",
      "Homophones for pr: []\n",
      "Homophones for hales: []\n",
      "Homophones for root: ['root', 'route']\n",
      "Homophones for batsman: ['batsman', 'batsmen']\n",
      "Homophones for bat: ['bat', 'batt']\n",
      "Homophones for peeled: ['peeled', 'pealed']\n",
      "Homophones for heap: []\n",
      "Homophones for nous: ['nous', 'noose']\n",
      "Homophones for rested: ['rested', 'wrested']\n",
      "Homophones for dependent: []\n",
      "----11600/14899----\n",
      "Homophones for bean: ['bean', 'been']\n",
      "Homophones for pal: []\n",
      "Homophones for tent: []\n",
      "Homophones for ally: []\n",
      "Homophones for kohl: ['kohl', 'coal', 'cole']\n",
      "Homophones for arch: []\n",
      "Homophones for straight: ['straight', 'strait']\n",
      "Homophones for ed: []\n",
      "Homophones for gorilla: ['gorilla', 'guerilla']\n",
      "----11700/14899----\n",
      "Homophones for phe: []\n",
      "Homophones for remit: []\n",
      "Homophones for serial: ['serial', 'cereal']\n",
      "----11800/14899----\n",
      "Homophones for waves: ['waves', 'waives']\n",
      "Homophones for bred: ['bred', 'bread']\n",
      "Homophones for ion: []\n",
      "Homophones for skull: ['skull', 'scull']\n",
      "Homophones for gene: ['gene', 'jean']\n",
      "Homophones for bus: ['bus', 'buss']\n",
      "Homophones for cruel: ['cruel', 'crewel']\n",
      "Homophones for packs: ['packs', 'pax']\n",
      "Homophones for keys: ['keys', 'cays', 'quays']\n",
      "----11900/14899----\n",
      "Homophones for ads: ['ads', 'adds', 'adze']\n",
      "Homophones for pro: []\n",
      "Homophones for lever: ['lever', 'leaver']\n",
      "Homophones for calendar: ['calendar', 'calender']\n",
      "Homophones for rs: []\n",
      "Homophones for babble: ['babble', 'babel']\n",
      "Homophones for plate: ['plate', 'plait']\n",
      "Homophones for flour: ['flour', 'flower']\n",
      "Homophones for knock: ['knock', 'nock']\n",
      "Homophones for stealing: ['stealing', 'steeling']\n",
      "----12000/14899----\n",
      "Homophones for steals: ['steals', 'steels', 'steles']\n",
      "Homophones for stole: []\n",
      "Homophones for bass: ['bass', 'base']\n",
      "Homophones for sword: ['sword', 'soared']\n",
      "Homophones for install: []\n",
      "Homophones for installation: ['installation', 'instillation']\n",
      "Homophones for apps: ['apps', 'apse']\n",
      "----12100/14899----\n",
      "Homophones for pall: ['pall', 'pol']\n",
      "Homophones for spa: []\n",
      "Homophones for ties: []\n",
      "Homophones for complement: ['complement', 'compliment']\n",
      "Homophones for throws: ['throws', 'throes']\n",
      "----12200/14899----\n",
      "Homophones for del: []\n",
      "Homophones for rey: []\n",
      "Homophones for tore: ['tore', 'tor', 'torr']\n",
      "Homophones for nose: ['nose', 'knows', 'noes']\n",
      "Homophones for tall: []\n",
      "Homophones for jeans: ['jeans', 'genes']\n",
      "Homophones for uta: []\n",
      "Homophones for assistants: ['assistants', 'assistance']\n",
      "----12300/14899----\n",
      "Homophones for we'll: [\"we'll\", 'weal', 'wheal', 'wheel']\n",
      "Homophones for sealing: ['sealing', 'ceiling']\n",
      "Homophones for maize: ['maize', 'maze']\n",
      "Homophones for sid: []\n",
      "Homophones for sec: []\n",
      "Homophones for lammer: []\n",
      "----12400/14899----\n",
      "Homophones for lowers: []\n",
      "Homophones for ups: []\n",
      "Homophones for aug: []\n",
      "Homophones for l: []\n",
      "Homophones for calculus: ['calculus', 'calculous']\n",
      "----12500/14899----\n",
      "Homophones for absent: []\n",
      "Homophones for d: ['d', 'dee']\n",
      "Homophones for s: ['s', 'ess']\n",
      "Homophones for taper: ['taper', 'tapir']\n",
      "Homophones for va: []\n",
      "Homophones for comb: []\n",
      "Homophones for fish: []\n",
      "Homophones for fillet: ['fillet', 'filet']\n",
      "Homophones for fry: []\n",
      "Homophones for you'll: [\"you'll\", 'yule']\n",
      "----12600/14899----\n",
      "Homophones for precedent: []\n",
      "Homophones for eder: []\n",
      "Homophones for fisher: ['fisher', 'fissure']\n",
      "Homophones for lech: []\n",
      "----12700/14899----\n",
      "Homophones for queues: ['queues', 'cues']\n",
      "Homophones for waived: ['waived', 'waved']\n",
      "Homophones for inn: ['inn', 'in']\n",
      "Homophones for carries: ['carries', 'caries']\n",
      "Homophones for pit: []\n",
      "Homophones for ferry: ['ferry', 'ferri-']\n",
      "----12800/14899----\n",
      "Homophones for sp: []\n",
      "Homophones for s&t: []\n",
      "Homophones for leased: ['leased', 'least']\n",
      "Homophones for ag: []\n",
      "Homophones for lia: []\n",
      "Homophones for bologna: ['bologna', 'baloney']\n",
      "Homophones for wine: ['wine', 'whine']\n",
      "----12900/14899----\n",
      "Homophones for descendant: ['descendant', 'descendent']\n",
      "Homophones for fuse: []\n",
      "Homophones for chase: []\n",
      "----13000/14899----\n",
      "Homophones for wood: ['wood', 'would']\n",
      "Homophones for lana: []\n",
      "Homophones for wins: ['wins', 'winze']\n",
      "Homophones for clue: ['clue', 'clew']\n",
      "Homophones for airs: []\n",
      "Homophones for nfl: []\n",
      "Homophones for wise: ['wise', 'whys', 'wyes']\n",
      "Homophones for cam: []\n",
      "----13100/14899----\n",
      "Homophones for static: []\n",
      "Homophones for phasing: ['phasing', 'fazing']\n",
      "Homophones for eon: []\n",
      "Homophones for rex: ['rex', 'wrecks']\n",
      "Homophones for plains: ['plains', 'planes']\n",
      "Homophones for alte: []\n",
      "Homophones for grosse: []\n",
      "Homophones for sale: ['sale', 'sail']\n",
      "----13200/14899----\n",
      "Homophones for weather: ['weather', 'wether', 'whether']\n",
      "Homophones for plumb: ['plumb', 'plum']\n",
      "Homophones for body's: [\"body's\", 'bodies']\n",
      "Homophones for sleeve: ['sleeve', 'sleave']\n",
      "----13300/14899----\n",
      "Homophones for lama: ['lama', 'llama']\n",
      "Homophones for rt: []\n",
      "Homophones for continents: ['continents', 'continence']\n",
      "Homophones for scents: ['scents', 'cense', 'cents', 'sense']\n",
      "Homophones for idle: ['idle', 'idol', 'idyll']\n",
      "Homophones for towed: ['towed', 'toad', 'toed']\n",
      "----13400/14899----\n",
      "Homophones for mill: ['mill', 'mil']\n",
      "Homophones for boer: ['boer', 'boar', 'bore']\n",
      "Homophones for ugly: ['ugly', 'ugli']\n",
      "Homophones for ons: []\n",
      "----13500/14899----\n",
      "Homophones for coax: ['coax', 'cokes']\n",
      "Homophones for wet: ['wet', 'whet']\n",
      "Homophones for booze: ['booze', 'boos']\n",
      "Homophones for incompetent: []\n",
      "Homophones for rider: []\n",
      "----13600/14899----\n",
      "Homophones for arse: []\n",
      "Homophones for halves: ['halves', 'haves']\n",
      "Homophones for tracked: ['tracked', 'tract']\n",
      "----13700/14899----\n",
      "Homophones for sick: ['sick', 'sic']\n",
      "Homophones for rape: []\n",
      "Homophones for torque: ['torque', 'torc']\n",
      "Homophones for ds: []\n",
      "Homophones for grille: ['grille', 'grill']\n",
      "Homophones for ref: []\n",
      "----13800/14899----\n",
      "Homophones for nsu: []\n",
      "Homophones for borde: []\n",
      "Homophones for hiv: []\n",
      "----13900/14899----\n",
      "Homophones for tu: []\n",
      "Homophones for flair: ['flair', 'flare']\n",
      "Homophones for craft: ['craft', 'kraft']\n",
      "Homophones for humorous: ['humorous', 'humerus']\n",
      "Homophones for pole: ['pole', 'poll']\n",
      "Homophones for lin: []\n",
      "Homophones for nr: []\n",
      "----14000/14899----\n",
      "Homophones for tech: ['tech', 'tec']\n",
      "Homophones for heal: ['heal', \"he'll\", 'heel']\n",
      "Homophones for premier: ['premier', 'premiere']\n",
      "Homophones for males: ['males', 'mails']\n",
      "Homophones for shone: ['shone', 'shown']\n",
      "Homophones for #friendsfest: []\n",
      "Homophones for di: []\n",
      "Homophones for swat: ['swat', 'swot']\n",
      "----14100/14899----\n",
      "Homophones for pence: []\n",
      "Homophones for ore: ['ore', 'oar', 'or']\n",
      "----14200/14899----\n",
      "Homophones for hoo: ['hoo', 'who']\n",
      "Homophones for urge: []\n",
      "Homophones for dear: ['dear', 'deer']\n",
      "Homophones for color: ['color', 'culler']\n",
      "Homophones for weed: ['weed', \"we'd\"]\n",
      "----14300/14899----\n",
      "Homophones for tacked: ['tacked', 'tact']\n",
      "Homophones for bunt: ['bunt', 'bundt']\n",
      "Homophones for rbi: []\n",
      "Homophones for rusty: []\n",
      "Homophones for phil: []\n",
      "Homophones for i'd: []\n",
      "Homophones for dl: []\n",
      "Homophones for wrap: ['wrap', 'rap']\n",
      "----14400/14899----\n",
      "Homophones for cale: []\n",
      "Homophones for b: ['b', 'be', 'bee']\n",
      "Homophones for c: ['c', 'cee', 'sea', 'see']\n",
      "----14500/14899----\n",
      "Homophones for uri: []\n",
      "Homophones for summery: ['summery', 'summary']\n",
      "Homophones for wind: ['wind', 'whined', 'wined', 'wynd']\n",
      "Homophones for alt: []\n",
      "Homophones for canvas: ['canvas', 'canvass']\n",
      "Homophones for prayer: []\n",
      "----14600/14899----\n",
      "Homophones for hans: []\n",
      "Homophones for pause: ['pause', 'paws']\n",
      "Homophones for #refugeeswelcome: []\n",
      "Homophones for v: []\n",
      "----14700/14899----\n",
      "Homophones for accidents: ['accidents', 'accidence']\n",
      "Homophones for boll: ['boll', 'bole', 'bowl']\n",
      "Homophones for seed: ['seed', 'cede']\n",
      "Homophones for pupil: ['pupil', 'pupal']\n",
      "Homophones for bras: []\n",
      "Homophones for lace: []\n",
      "Homophones for bear: ['bear', 'bair', 'bare']\n",
      "----14800/14899----\n",
      "Homophones for ute: []\n",
      "Homophones for arts: []\n",
      "Homophones for li: ['li', 'lea', 'lee']\n",
      "Homophones for liu: []\n",
      "Homophones for tighten: ['tighten', 'titan']\n",
      "Homophones for prey: ['prey', 'pray']\n",
      "Homophones for carp: []\n"
     ]
    }
   ],
   "source": [
    "#now we will add the homophones to the words dict\n",
    "counter = 0\n",
    "for key in words.keys():\n",
    "    # we need to try and except because some words dont have homophones\n",
    "    try:\n",
    "        words[key] = Pyphones(key).get_the_homophones()\n",
    "        print(\"Homophones for \" + key + \": \" + str(words[key]))\n",
    "    except:\n",
    "        words[key] = []\n",
    "        #print(\"No homophones for: \" + str(key) +str(words[key]))\n",
    "    #lets update the progress every 1000 words\n",
    "    if counter % 100 == 0:\n",
    "        #also lets print the word we are on and its homophones\n",
    "        print(\"----\" + str(counter) + \"/\" + str(len(words))+ \"----\") \n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the words dict to a pickle file\n",
    "import pickle\n",
    "with open('data/homophones_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(words, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spectacular': [],\n",
       " 'wingsuit': [],\n",
       " 'jump': [],\n",
       " 'over': [],\n",
       " 'bogota': [],\n",
       " 'sportsman': [],\n",
       " 'jhonathan': [],\n",
       " 'florez': [],\n",
       " 'jumped': [],\n",
       " 'from': [],\n",
       " 'a': ['a', 'uh'],\n",
       " 'helicopter': [],\n",
       " 'above': [],\n",
       " 'the': ['the', 'thee'],\n",
       " 'capital': ['capital', 'capitol'],\n",
       " 'of': [],\n",
       " 'colombia': [],\n",
       " 'on': [],\n",
       " 'thursday': [],\n",
       " 'wearing': [],\n",
       " 'he': [],\n",
       " 'flew': ['flew', 'flu', 'flue'],\n",
       " 'past': ['past', 'passed'],\n",
       " 'famous': [],\n",
       " 'monserrate': [],\n",
       " 'sanctuary': [],\n",
       " 'at': [],\n",
       " '160km/h': [],\n",
       " 'is': [],\n",
       " 'located': [],\n",
       " 'an': [],\n",
       " 'altitude': [],\n",
       " '3000': [],\n",
       " 'meters': [],\n",
       " 'and': [],\n",
       " 'numerous': [],\n",
       " 'spectators': [],\n",
       " 'had': [],\n",
       " 'gathered': [],\n",
       " 'there': ['there', 'their', \"they're\"],\n",
       " 'to': ['to', 'too', 'two'],\n",
       " 'watch': [],\n",
       " 'his': [],\n",
       " 'exploit': [],\n",
       " 'black': [],\n",
       " 'box': ['box', 'bocks'],\n",
       " 'in': ['in', 'inn'],\n",
       " 'your': ['your', 'yore', \"you're\"],\n",
       " 'car': [],\n",
       " 'as': ['as', 'ass', 'asse'],\n",
       " \"america's\": [],\n",
       " 'road': ['road', 'rode', 'rowed'],\n",
       " 'planners': [],\n",
       " 'struggle': [],\n",
       " 'find': ['find', 'fined'],\n",
       " 'cash': ['cash', 'cache'],\n",
       " 'mend': [],\n",
       " 'crumbling': [],\n",
       " 'highway': [],\n",
       " 'system': [],\n",
       " 'many': [],\n",
       " 'are': ['are', 'air', 'aire', 'ayre', 'ere', 'err', 'eyre', 'heir'],\n",
       " 'beginning': [],\n",
       " 'see': ['see', 'c', 'cee', 'sea'],\n",
       " 'solution': [],\n",
       " 'little': [],\n",
       " 'that': [],\n",
       " 'fits': [],\n",
       " 'neatly': [],\n",
       " 'by': ['by', 'bye', 'bi', 'buy'],\n",
       " 'dashboard': [],\n",
       " 'devices': [],\n",
       " 'which': ['which', 'wich', 'witch'],\n",
       " 'track': [],\n",
       " 'every': [],\n",
       " 'mile': [],\n",
       " 'motorist': [],\n",
       " 'drives': [],\n",
       " 'transmit': [],\n",
       " 'information': [],\n",
       " 'bureaucrats': [],\n",
       " 'center': [],\n",
       " 'controversial': [],\n",
       " 'attempt': [],\n",
       " 'washington': [],\n",
       " 'state': [],\n",
       " 'planning': [],\n",
       " 'offices': [],\n",
       " 'overhaul': [],\n",
       " 'outdated': [],\n",
       " 'for': ['for', 'fore', 'four'],\n",
       " 'funding': [],\n",
       " 'major': [],\n",
       " 'roads': [],\n",
       " 'usually': [],\n",
       " 'dull': [],\n",
       " 'arena': [],\n",
       " 'has': [],\n",
       " 'suddenly': [],\n",
       " 'spawned': [],\n",
       " 'intense': ['intense', 'intents'],\n",
       " 'debate': [],\n",
       " 'colorful': [],\n",
       " 'alliances': [],\n",
       " 'libertarians': [],\n",
       " 'have': ['have', 'halve'],\n",
       " 'joined': [],\n",
       " 'environmental': [],\n",
       " 'groups': [],\n",
       " 'lobbying': [],\n",
       " 'allow': [],\n",
       " 'government': [],\n",
       " 'use': ['use', 'ewes', 'yews'],\n",
       " 'boxes': [],\n",
       " 'keep': [],\n",
       " 'miles': [],\n",
       " 'you': ['you', 'ewe', 'yew'],\n",
       " 'drive': [],\n",
       " 'possibly': [],\n",
       " 'where': ['where', 'ware', 'wear', 'weir'],\n",
       " 'them': [],\n",
       " '-': [],\n",
       " 'then': [],\n",
       " 'draw': [],\n",
       " 'up': [],\n",
       " 'tax': ['tax', 'tacks'],\n",
       " 'bill': [],\n",
       " 'tea': ['tea', 't', 'tee', 'ti'],\n",
       " 'party': [],\n",
       " 'aghast': [],\n",
       " 'american': [],\n",
       " 'civil': [],\n",
       " 'liberties': [],\n",
       " 'union': [],\n",
       " 'deeply': [],\n",
       " 'concerned': [],\n",
       " 'too': ['too', 'to', 'two'],\n",
       " 'raising': ['raising', 'rasing', 'razing'],\n",
       " 'variety': [],\n",
       " 'privacy': [],\n",
       " 'issues': [],\n",
       " 'while': ['while', 'wile'],\n",
       " 'congress': [],\n",
       " \"can't\": [\"can't\", 'cant'],\n",
       " 'agree': [],\n",
       " 'whether': ['whether', 'weather', 'wether'],\n",
       " 'proceed': [],\n",
       " 'several': [],\n",
       " 'states': [],\n",
       " 'not': ['not', 'knot'],\n",
       " 'waiting': ['waiting', 'weighting'],\n",
       " 'they': [],\n",
       " 'exploring': [],\n",
       " 'how': [],\n",
       " 'next': [],\n",
       " 'decade': [],\n",
       " 'can': [],\n",
       " 'move': [],\n",
       " 'drivers': [],\n",
       " 'pay': [],\n",
       " 'per': ['per', 'purr'],\n",
       " 'roll': ['roll', 'role'],\n",
       " 'thousands': [],\n",
       " 'motorists': [],\n",
       " 'already': [],\n",
       " 'taken': [],\n",
       " 'some': ['some', 'sum'],\n",
       " 'gps': [],\n",
       " 'monitoring': [],\n",
       " 'test': [],\n",
       " 'this': [],\n",
       " 'really': [],\n",
       " 'must': ['must', 'mussed'],\n",
       " 'our': ['our', 'hour'],\n",
       " 'nation': [],\n",
       " 'it': [],\n",
       " 'matter': [],\n",
       " 'something': [],\n",
       " 'we': ['we', 'wee', 'whee'],\n",
       " 'might': ['might', 'mite'],\n",
       " 'choose': ['choose', 'chews'],\n",
       " 'do': ['do', 'dew', 'due'],\n",
       " 'said': ['said', 'sed'],\n",
       " 'hasan': [],\n",
       " 'ikhrata': [],\n",
       " 'executive': [],\n",
       " 'director': [],\n",
       " 'southern': [],\n",
       " 'california': [],\n",
       " 'assn': [],\n",
       " 'governments': [],\n",
       " 'start': [],\n",
       " 'tracking': [],\n",
       " 'driven': [],\n",
       " '2025': [],\n",
       " 'going': [],\n",
       " 'be': ['be', 'bee', 'b'],\n",
       " 'change': [],\n",
       " 'these': [],\n",
       " 'taxes': [],\n",
       " 'technology': [],\n",
       " 'push': [],\n",
       " 'comes': [],\n",
       " \"country's\": [],\n",
       " 'trust': ['trust', 'trussed'],\n",
       " 'fund': [],\n",
       " 'financed': [],\n",
       " 'with': [],\n",
       " 'americans': [],\n",
       " 'gas': [],\n",
       " 'pump': [],\n",
       " 'broke': [],\n",
       " \"don't\": [],\n",
       " 'buy': ['buy', 'bi', 'by', 'bye'],\n",
       " 'much': ['much', 'mutch'],\n",
       " 'used': [],\n",
       " 'cars': [],\n",
       " 'get': [],\n",
       " 'more': [],\n",
       " 'gallon': [],\n",
       " 'federal': [],\n",
       " 'itself': [],\n",
       " '18.4': [],\n",
       " 'cents': ['cents', 'scents', 'cense', 'sense'],\n",
       " \"hasn't\": [],\n",
       " 'gone': [],\n",
       " '20': [],\n",
       " 'years': [],\n",
       " 'politicians': [],\n",
       " 'loath': ['loath', 'loathe'],\n",
       " 'raise': ['raise', 'rase', 'rays', 'raze', 'res'],\n",
       " 'even': [],\n",
       " 'one': ['one', 'won'],\n",
       " 'penny': ['penny', 'penni'],\n",
       " 'when': ['when', 'wen'],\n",
       " 'prices': [],\n",
       " 'high': ['high', 'heigh', 'hi', 'hie'],\n",
       " 'just': [],\n",
       " 'sustainable': [],\n",
       " 'lee': ['lee', 'lea', 'li'],\n",
       " 'munnich': [],\n",
       " 'transportation': [],\n",
       " 'policy': [],\n",
       " 'expert': [],\n",
       " 'university': [],\n",
       " 'minnesota': [],\n",
       " 'recently': [],\n",
       " 'put': [],\n",
       " '500': [],\n",
       " 'out': [],\n",
       " 'pay-by-mile': [],\n",
       " 'works': [],\n",
       " 'most': [],\n",
       " 'logical': [],\n",
       " 'alternative': [],\n",
       " 'long': [],\n",
       " 'term': [],\n",
       " 'wonks': [],\n",
       " 'call': ['call', 'caul', 'col'],\n",
       " 'mileage-based': [],\n",
       " 'user': [],\n",
       " 'fee': [],\n",
       " 'no': ['no', 'know'],\n",
       " 'surprise': [],\n",
       " 'idea': [],\n",
       " 'appeals': [],\n",
       " 'urban': [],\n",
       " 'liberals': [],\n",
       " 'could': [],\n",
       " 'rigged': [],\n",
       " 'driving': [],\n",
       " 'patterns': [],\n",
       " 'ways': ['ways', 'weighs'],\n",
       " 'help': [],\n",
       " 'reduce': [],\n",
       " 'congestion': [],\n",
       " 'greenhouse': [],\n",
       " 'gases': [],\n",
       " 'example': [],\n",
       " 'looking': [],\n",
       " 'devise': [],\n",
       " 'strategies': [],\n",
       " 'meet': ['meet', 'meat', 'mete'],\n",
       " 'goals': [],\n",
       " 'laid': ['laid', 'lade', 'layed'],\n",
       " \"state's\": [],\n",
       " 'ambitious': [],\n",
       " 'global': [],\n",
       " 'warming': [],\n",
       " 'laws': [],\n",
       " 'but': ['but', 'butt'],\n",
       " 'rep': [],\n",
       " 'shuster': [],\n",
       " 'r-pa': [],\n",
       " 'chairman': [],\n",
       " 'house': ['house', \"how's\", 'hows'],\n",
       " 'committee': [],\n",
       " 'sees': ['sees', \"c's\", 'cees', 'seas', 'seize'],\n",
       " 'viable': [],\n",
       " 'long-term': [],\n",
       " 'free': [],\n",
       " 'marketeers': [],\n",
       " 'reason': [],\n",
       " 'foundation': [],\n",
       " 'also': [],\n",
       " 'fond': [],\n",
       " 'having': [],\n",
       " 'into': [],\n",
       " 'hole': ['hole', 'whole'],\n",
       " 'adrian': [],\n",
       " 'moore': [],\n",
       " 'vice': ['vice', 'vise'],\n",
       " 'president': [],\n",
       " 'people': [],\n",
       " 'paying': [],\n",
       " 'directly': [],\n",
       " 'what': ['what', 'watt', 'wot'],\n",
       " 'getting': [],\n",
       " 'movement': [],\n",
       " 'bolstered': [],\n",
       " 'two': ['two', 'to', 'too'],\n",
       " 'former': [],\n",
       " 'u.s': [],\n",
       " 'secretaries': [],\n",
       " 'who': ['who', 'hoo'],\n",
       " '2011': [],\n",
       " 'report': [],\n",
       " 'urged': [],\n",
       " 'pay-per-mile': [],\n",
       " 'direction': [],\n",
       " 'senate': ['senate', 'sennet', 'sennit'],\n",
       " 'approved': [],\n",
       " '$90-million': [],\n",
       " 'pilot': [],\n",
       " 'project': [],\n",
       " 'last': [],\n",
       " 'year': [],\n",
       " 'would': ['would', 'wood'],\n",
       " 'involved': [],\n",
       " 'about': [],\n",
       " '10,000': [],\n",
       " 'leadership': [],\n",
       " 'killed': [],\n",
       " 'proposal': [],\n",
       " 'acting': [],\n",
       " 'concerns': [],\n",
       " 'rural': [],\n",
       " 'lawmakers': [],\n",
       " 'representing': [],\n",
       " 'constituents': [],\n",
       " 'whose': ['whose', \"who's\"],\n",
       " 'daily': [],\n",
       " 'lives': [],\n",
       " 'often': [],\n",
       " 'involve': [],\n",
       " 'logging': [],\n",
       " 'lots': [],\n",
       " 'work': [],\n",
       " 'or': ['or', 'ore', 'oar'],\n",
       " 'town': [],\n",
       " 'cities': [],\n",
       " 'nonetheless': [],\n",
       " 'moving': [],\n",
       " 'ahead': [],\n",
       " 'their': ['their', 'there', \"they're\"],\n",
       " 'own': [],\n",
       " 'eager': [],\n",
       " 'oregon': [],\n",
       " 'enlisting': [],\n",
       " '5,000': [],\n",
       " 'biggest': [],\n",
       " 'experiment': [],\n",
       " 'those': [],\n",
       " 'will': [],\n",
       " 'soon': [],\n",
       " 'mileage': [],\n",
       " 'fees': ['fees', 'feaze', 'feeze'],\n",
       " 'instead': [],\n",
       " 'nevada': [],\n",
       " 'completed': [],\n",
       " 'new': ['new', 'knew', 'gnu', 'nu'],\n",
       " 'york': [],\n",
       " 'city': [],\n",
       " 'illinois': [],\n",
       " 'trying': [],\n",
       " 'limited': [],\n",
       " 'basis': [],\n",
       " 'trucks': [],\n",
       " 'i-95': [],\n",
       " 'coalition': [],\n",
       " 'includes': [],\n",
       " '17': [],\n",
       " 'departments': [],\n",
       " 'along': [],\n",
       " 'eastern': [],\n",
       " 'seaboard': [],\n",
       " 'including': [],\n",
       " 'maryland': [],\n",
       " 'pennsylvania': [],\n",
       " 'virginia': [],\n",
       " 'florida': [],\n",
       " 'studying': [],\n",
       " 'go': [],\n",
       " 'implementing': [],\n",
       " 'concept': [],\n",
       " 'universal': [],\n",
       " 'hit': [],\n",
       " '50': [],\n",
       " \"volunteers'\": [],\n",
       " 'were': ['were', 'whir'],\n",
       " 'equipped': [],\n",
       " 'ago': [],\n",
       " 'uneasy': [],\n",
       " 'being': [],\n",
       " 'able': ['able', 'Abel'],\n",
       " 'monitor': [],\n",
       " 'big': [],\n",
       " 'brother': [],\n",
       " 'sorts': [],\n",
       " 'things': [],\n",
       " 'problem': [],\n",
       " 'alauddin': [],\n",
       " 'khan': ['khan', 'con', 'conn'],\n",
       " 'directs': [],\n",
       " 'strategic': [],\n",
       " 'performance': [],\n",
       " 'management': [],\n",
       " 'department': [],\n",
       " 'was': [],\n",
       " 'wanted': [],\n",
       " 'trial': [],\n",
       " 'got': [],\n",
       " 'underway': [],\n",
       " 'aclu': [],\n",
       " 'warned': [],\n",
       " 'its': ['its', \"it's\"],\n",
       " 'website': [],\n",
       " 'fairly': [],\n",
       " 'easy': [],\n",
       " 'turn': ['turn', 'tern', 'terne'],\n",
       " 'full-fledged': [],\n",
       " 'need': ['need', 'kneed', 'knead'],\n",
       " 'build': ['build', 'billed'],\n",
       " 'enormous': [],\n",
       " 'unwieldy': [],\n",
       " 'technological': [],\n",
       " 'infrastructure': [],\n",
       " 'inevitably': [],\n",
       " 'expanded': [],\n",
       " 'records': [],\n",
       " \"individuals'\": [],\n",
       " 'everyday': [],\n",
       " 'comings': [],\n",
       " 'goings': [],\n",
       " 'among': [],\n",
       " 'now': [],\n",
       " 'scrambling': [],\n",
       " 'affordable': [],\n",
       " 'exactly': [],\n",
       " 'time': ['time', 'thyme'],\n",
       " 'if': [],\n",
       " 'public': [],\n",
       " 'gets': [],\n",
       " 'comfortable': [],\n",
       " 'hunt': [],\n",
       " 'led': ['led', 'lead'],\n",
       " 'agencies': [],\n",
       " 'small': [],\n",
       " 'startup': [],\n",
       " 'called': [],\n",
       " 'true': [],\n",
       " 'firm': [],\n",
       " 'originally': [],\n",
       " 'business': [],\n",
       " 'helping': [],\n",
       " 'seeking': [],\n",
       " 'break': ['break', 'brake'],\n",
       " 'emerging': ['emerging', 'immerging'],\n",
       " 'market': [],\n",
       " 'auto': [],\n",
       " 'insurance': [],\n",
       " 'based': ['based', 'baste'],\n",
       " 'testing': [],\n",
       " 'appeal': [],\n",
       " 'because': [],\n",
       " 'deliver': [],\n",
       " 'amount': [],\n",
       " 'uploaded': [],\n",
       " 'periodically': [],\n",
       " 'modem': [],\n",
       " 'willing': [],\n",
       " 'speed': [],\n",
       " 'location': [],\n",
       " 'ryan': [],\n",
       " 'morrison': [],\n",
       " 'chief': [],\n",
       " 'been': ['been', 'bean'],\n",
       " 'mistakes': [],\n",
       " 'programs': [],\n",
       " 'lot': [],\n",
       " 'less': ['less', 'loess'],\n",
       " 'expensive': [],\n",
       " 'intrusive': [],\n",
       " 'experimenting': [],\n",
       " 'giving': [],\n",
       " 'different': [],\n",
       " 'choices': [],\n",
       " 'device': [],\n",
       " 'without': [],\n",
       " 'all': ['all', 'awl'],\n",
       " 'opting': [],\n",
       " 'flat': [],\n",
       " 'average': [],\n",
       " 'number': [],\n",
       " 'residents': [],\n",
       " 'other': [],\n",
       " 'places': [],\n",
       " 'hoping': [],\n",
       " 'sell': ['sell', 'cel', 'cell'],\n",
       " 'wary': [],\n",
       " 'officials': [],\n",
       " 'develop': [],\n",
       " 'taxing': [],\n",
       " 'parking': [],\n",
       " 'meter': ['meter', 'meeter'],\n",
       " 'provide': [],\n",
       " 'pay-as-you-drive': [],\n",
       " 'create': [],\n",
       " 'pool': [],\n",
       " 'real-time': [],\n",
       " 'data': [],\n",
       " 'avoid': [],\n",
       " 'traffic': [],\n",
       " 'attracted': [],\n",
       " 'participate': [],\n",
       " 'value': [],\n",
       " 'benefits': [],\n",
       " 'offers': [],\n",
       " 'says': [],\n",
       " 'document': [],\n",
       " 'though': [],\n",
       " 'wonder': [],\n",
       " 'talk': [],\n",
       " 'giant': [],\n",
       " 'distraction': [],\n",
       " 'metropolitan': [],\n",
       " 'commission': [],\n",
       " 'san': [],\n",
       " 'francisco': [],\n",
       " 'bay': ['bay', 'bey'],\n",
       " 'area': [],\n",
       " 'say': ['say', 'se'],\n",
       " 'very': [],\n",
       " 'simply': [],\n",
       " 'deal': ['deal', 'dele'],\n",
       " 'bankrupt': [],\n",
       " 'extra': [],\n",
       " 'one-time': [],\n",
       " 'annual': [],\n",
       " 'levy': ['levy', 'levee'],\n",
       " 'imposed': [],\n",
       " 'hybrids': [],\n",
       " 'others': [],\n",
       " 'vehicles': [],\n",
       " 'so': ['so', 'sow', 'sew'],\n",
       " 'fair': ['fair', 'fare'],\n",
       " 'share': [],\n",
       " 'radical': ['radical', 'radicle'],\n",
       " 'surgery': [],\n",
       " 'take': [],\n",
       " 'aspirin': [],\n",
       " 'randy': [],\n",
       " 'rentschler': [],\n",
       " \"commission's\": [],\n",
       " 'legislation': [],\n",
       " 'affairs': [],\n",
       " 'hundreds': [],\n",
       " 'millions': [],\n",
       " 'host': [],\n",
       " 'david': [],\n",
       " 'bowie': [],\n",
       " 'four': ['four', 'for', 'fore'],\n",
       " 'unpublished': [],\n",
       " 'songs': [],\n",
       " 'released': [],\n",
       " 'british': [],\n",
       " 'musician': [],\n",
       " 'full': [],\n",
       " 'surprises': [],\n",
       " 'following': [],\n",
       " 'day': [],\n",
       " 'january': [],\n",
       " 'together': [],\n",
       " 'deluxe': [],\n",
       " 're-release': [],\n",
       " 'planned': [],\n",
       " 'november': [],\n",
       " '04': [],\n",
       " 'featuring': [],\n",
       " 'tracks': [],\n",
       " 'appeared': [],\n",
       " 'internet': [],\n",
       " 'announcement': [],\n",
       " 'releasing': [],\n",
       " 'album': [],\n",
       " 'stunned': [],\n",
       " 'world': ['world', 'whirled'],\n",
       " '08': [],\n",
       " '2013': [],\n",
       " 'date': [],\n",
       " '66th': [],\n",
       " 'birthday': [],\n",
       " 'announced': [],\n",
       " 'march': [],\n",
       " 'after': [],\n",
       " 'ten': [],\n",
       " 'silence': ['silence', 'silents'],\n",
       " 'record': [],\n",
       " 'reality': [],\n",
       " '2003': [],\n",
       " 'few': ['few', 'phew'],\n",
       " 'appearances': [],\n",
       " 'proved': [],\n",
       " 'still': [],\n",
       " 'light': ['light', 'lite'],\n",
       " 'pop': [],\n",
       " 'scene': ['scene', 'seen'],\n",
       " 'feast': [],\n",
       " 'fans': [],\n",
       " 'tired': [],\n",
       " 'making': [],\n",
       " 'than': [],\n",
       " 'trick': [],\n",
       " 'sleeves': [],\n",
       " 'thin': [],\n",
       " 'white': [],\n",
       " 'duke': [],\n",
       " 'real': ['real', 'reel'],\n",
       " 'mark': ['mark', 'marc', 'marque'],\n",
       " 'occasion': [],\n",
       " 'titled': [],\n",
       " 'presented': [],\n",
       " 'form': ['form', 'forme'],\n",
       " 'three': [],\n",
       " 'disks': [],\n",
       " 'original': [],\n",
       " 'studio': [],\n",
       " 'sessions': [],\n",
       " 'remixes': [],\n",
       " 'plus': [],\n",
       " 'dvd': [],\n",
       " 'containing': [],\n",
       " 'clips': [],\n",
       " 'unveiled': [],\n",
       " 'total': [],\n",
       " 'additional': [],\n",
       " 'compared': [],\n",
       " 'edition': [],\n",
       " 'five': [],\n",
       " 'specially': [],\n",
       " 'moreover': [],\n",
       " 'introduced': [],\n",
       " 'fine': [],\n",
       " 'box-set': [],\n",
       " 'through': ['through', 'threw', 'thru'],\n",
       " 'video': [],\n",
       " 'presents': ['presents', 'presence'],\n",
       " 'each': [],\n",
       " 'accessories': [],\n",
       " 'provided': [],\n",
       " 'exclusive': [],\n",
       " 'photos': [],\n",
       " 'notebook': [],\n",
       " 'sharing': [],\n",
       " 'impressions': [],\n",
       " 'booklet': [],\n",
       " 'lyrics': [],\n",
       " 'etc': [],\n",
       " 'finally': [],\n",
       " 'gives': [],\n",
       " 'teaser': [],\n",
       " 'atomica': [],\n",
       " 'typically': [],\n",
       " 'style': ['style', 'stile'],\n",
       " 'prominent': [],\n",
       " 'guitars': [],\n",
       " 'skillfully': [],\n",
       " 'controlled': [],\n",
       " 'rock': ['rock', 'roc'],\n",
       " 'electrics': [],\n",
       " 'previously': [],\n",
       " 'however': [],\n",
       " 'only': [],\n",
       " 'informer': [],\n",
       " 'like': [],\n",
       " 'rocket': [],\n",
       " 'man': [],\n",
       " 'born': ['born', 'bourn', 'bourne'],\n",
       " 'ufo': [],\n",
       " 'available': [],\n",
       " 'net': [],\n",
       " 'double-edged': [],\n",
       " 'unsettling': [],\n",
       " 'intro': [],\n",
       " 'followed': [],\n",
       " 'brilliant': [],\n",
       " 'rush': [],\n",
       " 'sound': [],\n",
       " 'progressively': [],\n",
       " 'slows': [],\n",
       " 'down': [],\n",
       " 'make': [],\n",
       " 'way': ['way', 'weigh', 'whey'],\n",
       " 'ballad': [],\n",
       " 'reference': [],\n",
       " 'elton': [],\n",
       " \"john's\": [],\n",
       " 'gravity': [],\n",
       " 'either': [],\n",
       " 'cheerful': [],\n",
       " 'singer': [],\n",
       " 'seems': ['seems', 'seams'],\n",
       " 'element': [],\n",
       " 'feet': ['feet', 'feat'],\n",
       " 'longer': [],\n",
       " 'ground': [],\n",
       " 'space': [],\n",
       " 'oddity': [],\n",
       " 'comparison': [],\n",
       " 'solemn': [],\n",
       " 'once': [],\n",
       " 'again': [],\n",
       " 'refers': [],\n",
       " 'strangeness': [],\n",
       " 'come': ['come', 'cum'],\n",
       " 'another': [],\n",
       " 'planet': [],\n",
       " 'spellbinding': [],\n",
       " 'guitar': [],\n",
       " 'riffs': [],\n",
       " 'want': [],\n",
       " 'leave': ['leave', 'lieve'],\n",
       " 'earth': [],\n",
       " 'any': [],\n",
       " 'case': [],\n",
       " 'enjoys': [],\n",
       " 'playing': [],\n",
       " 'chameleon': [],\n",
       " 'martian': [],\n",
       " 'veils': [],\n",
       " 'reveals': [],\n",
       " 'same': [],\n",
       " 'likes': [],\n",
       " 'personalities': [],\n",
       " 'throughout': [],\n",
       " 'career': [],\n",
       " 'notably': [],\n",
       " 'personas': [],\n",
       " 'ziggy': [],\n",
       " 'stardust': [],\n",
       " 'aladdin': [],\n",
       " 'sane': [],\n",
       " 'therefore': ['therefore', 'therefor'],\n",
       " 'surprising': [],\n",
       " 'should': [],\n",
       " 'holding': [],\n",
       " 'mask': ['mask', 'masque'],\n",
       " 'promotional': [],\n",
       " 'photography': [],\n",
       " \"l'invitation\": [],\n",
       " 'au': [],\n",
       " 'voyage': [],\n",
       " 'louis': [],\n",
       " 'vuitton': [],\n",
       " 'face': [],\n",
       " 'appears': [],\n",
       " 'adverts': [],\n",
       " 'broadcast': [],\n",
       " '10': [],\n",
       " 'minister': [],\n",
       " 'defence': [],\n",
       " 'rob': [],\n",
       " 'nicholson': [],\n",
       " 'insisted': [],\n",
       " 'injured': [],\n",
       " 'soldiers': [],\n",
       " 'summarily': [],\n",
       " 'discharged': [],\n",
       " 'canadian': [],\n",
       " 'armed': [],\n",
       " 'forces': [],\n",
       " 'stressed': [],\n",
       " 'undergo': [],\n",
       " 'transition': [],\n",
       " 'process': [],\n",
       " 'before': [],\n",
       " 'return': [],\n",
       " 'civilian': [],\n",
       " 'life': [],\n",
       " 'attacked': [],\n",
       " 'neo-democrats': [],\n",
       " 'commons': [],\n",
       " 'mr': [],\n",
       " 'assured': [],\n",
       " 'prior': ['prior', 'prier'],\n",
       " 'discharge': [],\n",
       " 'members': [],\n",
       " 'army': [],\n",
       " 'underwent': [],\n",
       " 'plan': [],\n",
       " 'collaboration': [],\n",
       " 'superiors': [],\n",
       " 'receive': [],\n",
       " 'appropriate': [],\n",
       " 'care': [],\n",
       " 'preparation': [],\n",
       " 'none': ['none', 'nun'],\n",
       " 'ready': [],\n",
       " 'asserted': [],\n",
       " 'detractors': [],\n",
       " 'accusing': [],\n",
       " 'save': [],\n",
       " 'money': [],\n",
       " 'allowing': [],\n",
       " \"army's\": [],\n",
       " 'rule': [],\n",
       " 'universality': [],\n",
       " 'service': [],\n",
       " 'requires': [],\n",
       " 'personnel': [],\n",
       " 'carry': [],\n",
       " 'series': [],\n",
       " 'varying': [],\n",
       " 'tasks': [],\n",
       " 'reach': [],\n",
       " 'ten-year': [],\n",
       " 'period': [],\n",
       " 'admissibility': [],\n",
       " 'required': [],\n",
       " 'retirement': [],\n",
       " 'specifically': [],\n",
       " 'noted': [],\n",
       " 'cases': [],\n",
       " 'reported': [],\n",
       " 'la': [],\n",
       " 'presse': [],\n",
       " 'canadienne': [],\n",
       " 'involving': [],\n",
       " 'soldier': [],\n",
       " 'friday': [],\n",
       " 'lance': ['lance', 'launce'],\n",
       " 'corporal': [],\n",
       " 'hawkins': [],\n",
       " 'reservist': [],\n",
       " 'london': [],\n",
       " 'ontario': [],\n",
       " 'diagnosed': [],\n",
       " 'post-traumatic': [],\n",
       " 'stress': [],\n",
       " 'disorder': [],\n",
       " 'despite': [],\n",
       " 'asking': [],\n",
       " 'kept': [],\n",
       " 'fully-indexed': [],\n",
       " 'pension': [],\n",
       " 'follows': [],\n",
       " 'glen': [],\n",
       " 'kirkland': [],\n",
       " 'declared': [],\n",
       " 'parliamentary': [],\n",
       " 'month': [],\n",
       " 'forced': [],\n",
       " 'did': [],\n",
       " 'prepared': [],\n",
       " 'departure': [],\n",
       " 'consultation': [],\n",
       " 'totally': [],\n",
       " 'wanting': [],\n",
       " 'i': [],\n",
       " 'told': ['told', 'tolled'],\n",
       " \"wasn't\": [],\n",
       " 'interview': [],\n",
       " 'wednesday': [],\n",
       " 'months': [],\n",
       " 'asked': [],\n",
       " 'stay': [],\n",
       " 'adds': ['adds', 'ads', 'adze'],\n",
       " 'since': [],\n",
       " 'combat': [],\n",
       " 'afghanistan': [],\n",
       " 'struggled': [],\n",
       " 'determine': [],\n",
       " 'latitude': [],\n",
       " 'give': [],\n",
       " 'capable': [],\n",
       " 'under': [],\n",
       " 'current': ['current', 'currant'],\n",
       " 'rules': [],\n",
       " 'seriously': [],\n",
       " 'recover': [],\n",
       " 'criteria': [],\n",
       " 'overseas': ['overseas', 'oversees'],\n",
       " 'deployment': [],\n",
       " 'parliament': [],\n",
       " 'indicates': [],\n",
       " '1,218': [],\n",
       " 'medical': [],\n",
       " 'reasons': [],\n",
       " '199': [],\n",
       " 'reached': [],\n",
       " 'length': [],\n",
       " 'obtain': [],\n",
       " 'liberal': [],\n",
       " 'spokesman': [],\n",
       " 'jim': [],\n",
       " 'karygiannis': [],\n",
       " 'reinstated': [],\n",
       " 'neo-democrat': [],\n",
       " 'jack': [],\n",
       " 'harris': [],\n",
       " 'demanded': [],\n",
       " 'immediate': [],\n",
       " 'end': [],\n",
       " 'shameful': [],\n",
       " 'practice': [],\n",
       " 'disney': [],\n",
       " 'launch': [],\n",
       " 'animated': [],\n",
       " 'tablet': [],\n",
       " 'pcs': [],\n",
       " 'media': [],\n",
       " 'entertainment': [],\n",
       " 'group': [],\n",
       " 'decided': [],\n",
       " 'priority': [],\n",
       " 'television': [],\n",
       " 'channels': [],\n",
       " 'release': [],\n",
       " 'children': [],\n",
       " 'first': [],\n",
       " 'nine': [],\n",
       " 'episodes': [],\n",
       " 'sheriff': [],\n",
       " \"callie's\": [],\n",
       " 'wild': ['wild', 'whiled'],\n",
       " 'west': [],\n",
       " '24': [],\n",
       " 'site': ['site', 'cite', 'sight'],\n",
       " 'watchdisneyjunior.com': [],\n",
       " 'via': [],\n",
       " 'application': [],\n",
       " 'mobile': [],\n",
       " 'phones': [],\n",
       " 'tablets': [],\n",
       " 'until': [],\n",
       " '2014': [],\n",
       " 'according': [],\n",
       " 'press': [],\n",
       " 'junior': [],\n",
       " 'division': [],\n",
       " 'animation': [],\n",
       " 'aimed': [],\n",
       " 'aged': [],\n",
       " '2': [],\n",
       " '7': [],\n",
       " 'adventures': [],\n",
       " 'cat': [],\n",
       " 'callie': [],\n",
       " 'she': ['she', 'sidhe'],\n",
       " 'keeps': [],\n",
       " 'law': [],\n",
       " 'order': [],\n",
       " 'using': [],\n",
       " 'magic': [],\n",
       " 'lasso': [],\n",
       " 'episode': [],\n",
       " 'contains': [],\n",
       " '11-minute': [],\n",
       " 'stories': [],\n",
       " 'interacting': [],\n",
       " 'smartphones': [],\n",
       " 'second': [],\n",
       " 'nature': [],\n",
       " 'today': [],\n",
       " 'notes': [],\n",
       " 'albert': [],\n",
       " 'cheng': [],\n",
       " 'vice-president': [],\n",
       " 'digital': [],\n",
       " 'products': [],\n",
       " 'disney/abc': [],\n",
       " 'quote': [],\n",
       " 'kind': [],\n",
       " 'experience': [],\n",
       " 'part': [],\n",
       " \"disney's\": [],\n",
       " 'efforts': [],\n",
       " 'extend': [],\n",
       " 'lifetime': [],\n",
       " 'relationships': [],\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create a dictionary of all letters and the letters that they could be confused with\n",
    "#for example, the letter \"a\" could be confused with the letter \"e\" because they look similar, same with b and d, etc.\n",
    "\n",
    "confusing_letters = {\"a\": [\"e\", \"i\", \"o\", \"u\"],\n",
    "            \"b\": [\"d\", \"p\", \"q\"],\n",
    "            \"c\": [\"e\", \"o\", \"s\"],\n",
    "            \"d\": [\"b\", \"p\", \"q\"],\n",
    "            \"e\": [\"a\", \"c\", \"o\"],\n",
    "            \"f\": [\"t\"],\n",
    "            \"g\": [\"q\"],\n",
    "            \"h\": [\"n\"],\n",
    "            \"i\": [\"j\", \"l\", \"o\", \"u\"],\n",
    "            \"j\": [\"i\", \"l\"],\n",
    "            \"k\": [\"x\"],\n",
    "            \"l\": [\"i\", \"j\"],\n",
    "            \"m\": [\"n\"],\n",
    "            \"n\": [\"h\", \"m\", \"u\"],\n",
    "            \"o\": [\"a\", \"c\", \"e\"],\n",
    "            \"p\": [\"b\", \"d\", \"q\"],\n",
    "            \"q\": [\"b\", \"d\", \"g\", \"p\"],\n",
    "            \"r\": [\"n\"],\n",
    "            \"s\": [\"c\"],\n",
    "            \"t\": [\"f\"],\n",
    "            \"u\": [\"i\", \"v\", \"n\"],\n",
    "            \"v\": [\"u\"],\n",
    "            \"w\": [\"v\"],\n",
    "            \"x\": [\"k\"],\n",
    "            \"y\": [\"v\"],\n",
    "            \"z\": [\"s\"]}\n",
    "#this list was created by me and copied from the following website: https://www.grammarly.com/blog/confusing-letters/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets save the confusing letters dict to a pickle file\n",
    "with open('data/confusing_letters_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(confusing_letters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spectacular Wingsuit Jump Over Bogota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sportsman Jhonathan Florez jumped from a helic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wearing a wingsuit, he flew past over the famo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A black box in your car?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As America's road planners struggle to find th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0              Spectacular Wingsuit Jump Over Bogota\n",
       "1  Sportsman Jhonathan Florez jumped from a helic...\n",
       "2  Wearing a wingsuit, he flew past over the famo...\n",
       "3                           A black box in your car?\n",
       "4  As America's road planners struggle to find th..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets load in our data from a csv file\n",
    "import pandas as pd\n",
    "df_wmt14_en = pd.read_csv(\"data/wmt14_english_data.csv\", header=None)\n",
    "df_wmt16_en = pd.read_csv(\"data/wmt16_english_data.csv\", header=None)\n",
    "\n",
    "df_wmt14_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we now need to define a function that with a probability p1, we will swap a word with a homophone and a probability p2, we will swap a letter with a confusing letter\n",
    "\n",
    "#we also want to return the amount of words that were swapped, and the amount of letters that were swapped, and the total amount of words changed\n",
    "import random\n",
    "\n",
    "def injection_swap(sentence, homophones_dict, confusing_letters_dict, p1=0, p2=0):\n",
    "    #first we need to split the sentence into a list of words\n",
    "    words = sentence.split()\n",
    "    #we need to keep track of the amount of words that were swapped\n",
    "    words_swapped = 0\n",
    "    #we also need to keep track of the amount of letters that were swapped\n",
    "    letters_swapped = 0\n",
    "    #we also need to keep track of the amount of words that were changed\n",
    "    words_changed = 0\n",
    "    for i in range(len(words)):\n",
    "        #if the word has any punctuation or capitalization, we skip it\n",
    "        if words[i].isalpha() == False:\n",
    "            continue\n",
    "        if words[i].islower() == False:\n",
    "            continue\n",
    "        word = words[i].lower().strip('\".,?!:;()')\n",
    "        #we need to check if the word is in the homophones dict\n",
    "        if word in homophones_dict.keys():\n",
    "            #we need to check if the word has any homophones\n",
    "            if len(homophones_dict[word]) > 0:\n",
    "                #we need to check if we will swap the word with a homophone with probability p1\n",
    "                if random.random() < p1:\n",
    "                    #we need to pick a random homophone from the list of homophones\n",
    "                    homophone = random.choice(homophones_dict[word])\n",
    "                    #we need to swap the word with the homophone\n",
    "                    words[i] = homophone\n",
    "                    #we need to update the amount of words that were swapped\n",
    "                    words_swapped += 1\n",
    "                    #we need to update the amount of words that were changed\n",
    "                    words_changed += 1\n",
    "        for j in range(len(word)):\n",
    "            #we need to check if we will swap a letter with a confusing letter\n",
    "            if random.random() < p2:\n",
    "                #we need to check if the word is in the confusing letters dict\n",
    "                if word[j] in confusing_letters_dict.keys():\n",
    "                    #we need to pick a random letter from the list of confusing letters\n",
    "                    confusing_letter = random.choice(confusing_letters_dict[word[j]])\n",
    "                    \n",
    "                    #replace the letter with the confusing letter at index j\n",
    "                    word = word[:j] + confusing_letter + word[j+1:]\n",
    "\n",
    "                    words[i] = word\n",
    "                    #we need to update the amount of letters that were swapped\n",
    "                    letters_swapped += 1\n",
    "                    #we need to update the amount of words that were changed\n",
    "                    words_changed += 1\n",
    "    #we need to join the list of words back into a sentence\n",
    "    sentence = \" \".join(words)\n",
    "    return sentence, (words_swapped, letters_swapped, words_changed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a sentence that has some words that will be swapped with homophones and some letters that will be swapped with confusing letters.\n",
      "I am uh scnfeuce fnef nuc came wonds tnet viil b svappod vlfn homeqnomoc unq same lcttorc fnif wiji bo cvappcq wutn confisung letters.\n",
      "Words swapped: 3\n",
      "Letters swapped: 50\n",
      "Words changed: 53\n"
     ]
    }
   ],
   "source": [
    "#lets test our function\n",
    "sentence = \"I am a sentence that has some words that will be swapped with homophones and some letters that will be swapped with confusing letters.\"\n",
    "print(sentence)\n",
    "sentence, results = injection_swap(sentence, words, confusing_letters, p1=0.5, p2=0.5)\n",
    "print(sentence)\n",
    "print(\"Words swapped: \" + str(results[0]))\n",
    "print(\"Letters swapped: \" + str(results[1]))\n",
    "print(\"Words changed: \" + str(results[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that takes in a dataframe and uses the injection_swap function to swap words and letters\n",
    "\n",
    "def injection_swap_df(df, homophones_dict, confusing_letters_dict, p1=0, p2=0):\n",
    "\n",
    "    #we need to keep track of the amount of words that were swapped\n",
    "    words_swapped = 0\n",
    "    #we also need to keep track of the amount of letters that were swapped\n",
    "    letters_swapped = 0\n",
    "    #we also need to keep track of the amount of words that were changed\n",
    "    words_changed = 0\n",
    "    #we need to keep track of the amount of sentences that were changed\n",
    "    sentences_changed = 0\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        #we need to get the sentence\n",
    "        sentence = df.iloc[i][0]\n",
    "        #we need to swap the sentence\n",
    "        sentence, results = injection_swap(sentence, homophones_dict, confusing_letters_dict, p1, p2)\n",
    "        #we need to update the amount of words that were swapped\n",
    "        words_swapped += results[0]\n",
    "        #we need to update the amount of letters that were swapped\n",
    "        letters_swapped += results[1]\n",
    "        #we need to update the amount of words that were changed\n",
    "        words_changed += results[2]\n",
    "        #we need to update the amount of sentences that were changed\n",
    "        if results[2] > 0:\n",
    "            sentences_changed += 1\n",
    "        #we need to update the sentence in the dataframe\n",
    "        df.iloc[i][0] = sentence\n",
    "\n",
    "    print(\"Words swapped: \" + str(words_swapped))\n",
    "    print(\"Letters swapped: \" + str(letters_swapped))\n",
    "    print(\"Words changed: \" + str(words_changed))\n",
    "    print(\"Sentences changed: \" + str(sentences_changed))\n",
    "    return df, (words_swapped, letters_swapped, words_changed, sentences_changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1: 0.005\n",
      "p2: 0.005\n",
      "Words swapped: 77\n",
      "Letters swapped: 0\n",
      "Words changed: 77\n",
      "Sentences changed: 74\n",
      "Words swapped: 0\n",
      "Letters swapped: 977\n",
      "Words changed: 977\n",
      "Sentences changed: 783\n",
      "Words swapped: 74\n",
      "Letters swapped: 1028\n",
      "Words changed: 1102\n",
      "Sentences changed: 883\n",
      "Words swapped: 92\n",
      "Letters swapped: 0\n",
      "Words changed: 92\n",
      "Sentences changed: 91\n",
      "Words swapped: 0\n",
      "Letters swapped: 955\n",
      "Words changed: 955\n",
      "Sentences changed: 787\n",
      "Words swapped: 85\n",
      "Letters swapped: 948\n",
      "Words changed: 1033\n",
      "Sentences changed: 819\n",
      "p1: 0.01\n",
      "p2: 0.01\n",
      "Words swapped: 166\n",
      "Letters swapped: 0\n",
      "Words changed: 166\n",
      "Sentences changed: 155\n",
      "Words swapped: 0\n",
      "Letters swapped: 2095\n",
      "Words changed: 2095\n",
      "Sentences changed: 1394\n",
      "Words swapped: 147\n",
      "Letters swapped: 2095\n",
      "Words changed: 2242\n",
      "Sentences changed: 1440\n",
      "Words swapped: 165\n",
      "Letters swapped: 0\n",
      "Words changed: 165\n",
      "Sentences changed: 158\n",
      "Words swapped: 0\n",
      "Letters swapped: 1868\n",
      "Words changed: 1868\n",
      "Sentences changed: 1285\n",
      "Words swapped: 144\n",
      "Letters swapped: 1932\n",
      "Words changed: 2076\n",
      "Sentences changed: 1399\n",
      "p1: 0.015\n",
      "p2: 0.015\n",
      "Words swapped: 229\n",
      "Letters swapped: 0\n",
      "Words changed: 229\n",
      "Sentences changed: 220\n",
      "Words swapped: 0\n",
      "Letters swapped: 3201\n",
      "Words changed: 3201\n",
      "Sentences changed: 1763\n",
      "Words swapped: 240\n",
      "Letters swapped: 3166\n",
      "Words changed: 3406\n",
      "Sentences changed: 1820\n",
      "Words swapped: 224\n",
      "Letters swapped: 0\n",
      "Words changed: 224\n",
      "Sentences changed: 216\n",
      "Words swapped: 0\n",
      "Letters swapped: 2777\n",
      "Words changed: 2777\n",
      "Sentences changed: 1654\n",
      "Words swapped: 223\n",
      "Letters swapped: 2799\n",
      "Words changed: 3022\n",
      "Sentences changed: 1713\n",
      "p1: 0.02\n",
      "p2: 0.02\n",
      "Words swapped: 323\n",
      "Letters swapped: 0\n",
      "Words changed: 323\n",
      "Sentences changed: 305\n",
      "Words swapped: 0\n",
      "Letters swapped: 4172\n",
      "Words changed: 4172\n",
      "Sentences changed: 2012\n",
      "Words swapped: 324\n",
      "Letters swapped: 4215\n",
      "Words changed: 4539\n",
      "Sentences changed: 2069\n",
      "Words swapped: 290\n",
      "Letters swapped: 0\n",
      "Words changed: 290\n",
      "Sentences changed: 274\n",
      "Words swapped: 0\n",
      "Letters swapped: 3815\n",
      "Words changed: 3815\n",
      "Sentences changed: 1962\n",
      "Words swapped: 278\n",
      "Letters swapped: 3703\n",
      "Words changed: 3981\n",
      "Sentences changed: 2010\n",
      "p1: 0.025\n",
      "p2: 0.025\n",
      "Words swapped: 375\n",
      "Letters swapped: 0\n",
      "Words changed: 375\n",
      "Sentences changed: 345\n",
      "Words swapped: 0\n",
      "Letters swapped: 5126\n",
      "Words changed: 5126\n",
      "Sentences changed: 2212\n",
      "Words swapped: 371\n",
      "Letters swapped: 5296\n",
      "Words changed: 5667\n",
      "Sentences changed: 2251\n",
      "Words swapped: 354\n",
      "Letters swapped: 0\n",
      "Words changed: 354\n",
      "Sentences changed: 327\n",
      "Words swapped: 0\n",
      "Letters swapped: 4776\n",
      "Words changed: 4776\n",
      "Sentences changed: 2105\n",
      "Words swapped: 405\n",
      "Letters swapped: 4624\n",
      "Words changed: 5029\n",
      "Sentences changed: 2159\n",
      "p1: 0.03\n",
      "p2: 0.03\n",
      "Words swapped: 488\n",
      "Letters swapped: 0\n",
      "Words changed: 488\n",
      "Sentences changed: 444\n",
      "Words swapped: 0\n",
      "Letters swapped: 6187\n",
      "Words changed: 6187\n",
      "Sentences changed: 2304\n",
      "Words swapped: 510\n",
      "Letters swapped: 6271\n",
      "Words changed: 6781\n",
      "Sentences changed: 2416\n",
      "Words swapped: 444\n",
      "Letters swapped: 0\n",
      "Words changed: 444\n",
      "Sentences changed: 410\n",
      "Words swapped: 0\n",
      "Letters swapped: 5578\n",
      "Words changed: 5578\n",
      "Sentences changed: 2293\n",
      "Words swapped: 442\n",
      "Letters swapped: 5687\n",
      "Words changed: 6129\n",
      "Sentences changed: 2360\n",
      "p1: 0.035\n",
      "p2: 0.035\n",
      "Words swapped: 584\n",
      "Letters swapped: 0\n",
      "Words changed: 584\n",
      "Sentences changed: 506\n",
      "Words swapped: 0\n",
      "Letters swapped: 7353\n",
      "Words changed: 7353\n",
      "Sentences changed: 2436\n",
      "Words swapped: 543\n",
      "Letters swapped: 7310\n",
      "Words changed: 7853\n",
      "Sentences changed: 2477\n",
      "Words swapped: 526\n",
      "Letters swapped: 0\n",
      "Words changed: 526\n",
      "Sentences changed: 461\n",
      "Words swapped: 0\n",
      "Letters swapped: 6572\n",
      "Words changed: 6572\n",
      "Sentences changed: 2387\n",
      "Words swapped: 510\n",
      "Letters swapped: 6426\n",
      "Words changed: 6936\n",
      "Sentences changed: 2428\n",
      "p1: 0.04\n",
      "p2: 0.04\n",
      "Words swapped: 599\n",
      "Letters swapped: 0\n",
      "Words changed: 599\n",
      "Sentences changed: 528\n",
      "Words swapped: 0\n",
      "Letters swapped: 8355\n",
      "Words changed: 8355\n",
      "Sentences changed: 2517\n",
      "Words swapped: 624\n",
      "Letters swapped: 8477\n",
      "Words changed: 9101\n",
      "Sentences changed: 2568\n",
      "Words swapped: 530\n",
      "Letters swapped: 0\n",
      "Words changed: 530\n",
      "Sentences changed: 485\n",
      "Words swapped: 0\n",
      "Letters swapped: 7603\n",
      "Words changed: 7603\n",
      "Sentences changed: 2484\n",
      "Words swapped: 566\n",
      "Letters swapped: 7502\n",
      "Words changed: 8068\n",
      "Sentences changed: 2530\n",
      "p1: 0.045\n",
      "p2: 0.045\n",
      "Words swapped: 748\n",
      "Letters swapped: 0\n",
      "Words changed: 748\n",
      "Sentences changed: 647\n",
      "Words swapped: 0\n",
      "Letters swapped: 9400\n",
      "Words changed: 9400\n",
      "Sentences changed: 2584\n",
      "Words swapped: 717\n",
      "Letters swapped: 9618\n",
      "Words changed: 10335\n",
      "Sentences changed: 2622\n",
      "Words swapped: 626\n",
      "Letters swapped: 0\n",
      "Words changed: 626\n",
      "Sentences changed: 542\n",
      "Words swapped: 0\n",
      "Letters swapped: 8510\n",
      "Words changed: 8510\n",
      "Sentences changed: 2571\n",
      "Words swapped: 645\n",
      "Letters swapped: 8362\n",
      "Words changed: 9007\n",
      "Sentences changed: 2570\n",
      "p1: 0.05\n",
      "p2: 0.05\n",
      "Words swapped: 784\n",
      "Letters swapped: 0\n",
      "Words changed: 784\n",
      "Sentences changed: 681\n",
      "Words swapped: 0\n",
      "Letters swapped: 10448\n",
      "Words changed: 10448\n",
      "Sentences changed: 2637\n",
      "Words swapped: 815\n",
      "Letters swapped: 10678\n",
      "Words changed: 11493\n",
      "Sentences changed: 2674\n",
      "Words swapped: 757\n",
      "Letters swapped: 0\n",
      "Words changed: 757\n",
      "Sentences changed: 641\n",
      "Words swapped: 0\n",
      "Letters swapped: 9373\n",
      "Words changed: 9373\n",
      "Sentences changed: 2611\n",
      "Words swapped: 723\n",
      "Letters swapped: 9518\n",
      "Words changed: 10241\n",
      "Sentences changed: 2637\n"
     ]
    }
   ],
   "source": [
    "#now lets apply our function to the wmt14 and wmt16 data and save the new data to a csv file\n",
    "\n",
    "#lets start with p1 = 0.005 and p2 = 0.005 then we increase p1 and p2 by 0.005 until we get to p1 = 0.05 and p2 = 0.05\n",
    "#also create a df to store the results of the injection swap for each iteration\n",
    "\n",
    "df_swap_results = pd.DataFrame(columns=[\"dataset\",\"p1\", \"p2\", \"words_swapped\", \"letters_swapped\", \"words_changed\", \"sentences_changed\"])\n",
    "\n",
    "for i in range(5, 55, 5):\n",
    "    p1 = i/1000\n",
    "    p2 = i/1000\n",
    "    print(\"p1: \" + str(p1))\n",
    "    print(\"p2: \" + str(p2))\n",
    "\n",
    "    \n",
    "    #also save as xlsx for amazon formating\n",
    "    df_wmt14_en_temp, results = injection_swap_df(df_wmt14_en.copy(deep=True), words, confusing_letters, p1=p1, p2=0)\n",
    "    df_wmt14_en_temp.to_csv(\"data/injected/wmt14/wmt14_english_data_injection_swap_p1_\" + str(p1) + \".csv\", header=False, index=False)\n",
    "    df_swap_results.loc[len(df_swap_results)] = {\"dataset\":\"wmt14_en\", \"p1\":p1, \"p2\":0, \"words_swapped\":results[0], \"letters_swapped\":results[1], \"words_changed\":results[2], \"sentences_changed\":results[3]}\n",
    "    df_wmt14_en_temp.to_excel(\"data/injected/for_amazon/wmt14_english_data_injection_swap_p1_\" + str(p1) + \".xlsx\", header=False, index=False)\n",
    "\n",
    "    df_wmt14_en_temp, results = injection_swap_df(df_wmt14_en.copy(deep=True), words, confusing_letters, p1=0, p2=p2)\n",
    "    df_wmt14_en_temp.to_csv(\"data/injected/wmt14/wmt14_english_data_injection_swap_p2_\" + str(p2) + \".csv\", header=False, index=False)\n",
    "    df_swap_results.loc[len(df_swap_results)] = {\"dataset\":\"wmt14_en\", \"p1\":0, \"p2\":p2, \"words_swapped\":results[0], \"letters_swapped\":results[1], \"words_changed\":results[2], \"sentences_changed\":results[3]}\n",
    "    df_wmt14_en_temp.to_excel(\"data/injected/for_amazon/wmt14_english_data_injection_swap_p2_\" + str(p2) + \".xlsx\", header=False, index=False)\n",
    "\n",
    "    df_wmt14_en_temp, results = injection_swap_df(df_wmt14_en.copy(deep=True), words, confusing_letters, p1=p1, p2=p2)\n",
    "    df_wmt14_en_temp.to_csv(\"data/injected/wmt14/wmt14_english_data_injection_swap_p1_\" + str(p1) + \"_p2_\" + str(p2) + \".csv\", header=False, index=False)\n",
    "    df_swap_results.loc[len(df_swap_results)] = {\"dataset\":\"wmt14_en\", \"p1\":p1, \"p2\":p2, \"words_swapped\":results[0], \"letters_swapped\":results[1], \"words_changed\":results[2], \"sentences_changed\":results[3]}\n",
    "    df_wmt14_en_temp.to_excel(\"data/injected/for_amazon/wmt14_english_data_injection_swap_p1_\" + str(p1) + \"_p2_\" + str(p2) + \".xlsx\", header=False, index=False)\n",
    "\n",
    "    df_wmt16_en_temp, results = injection_swap_df(df_wmt16_en.copy(deep=True), words, confusing_letters, p1=p1, p2=0)\n",
    "    df_wmt16_en_temp.to_csv(\"data/injected/wmt16/wmt16_english_data_injection_swap_p1_\" + str(p1) + \".csv\", header=False, index=False)\n",
    "    df_swap_results.loc[len(df_swap_results)] = {\"dataset\":\"wmt16_en\", \"p1\":p1, \"p2\":0, \"words_swapped\":results[0], \"letters_swapped\":results[1], \"words_changed\":results[2], \"sentences_changed\":results[3]}\n",
    "    df_wmt16_en_temp.to_excel(\"data/injected/for_amazon/wmt16_english_data_injection_swap_p1_\" + str(p1) + \".xlsx\", header=False, index=False)\n",
    "\n",
    "    df_wmt16_en_temp, results = injection_swap_df(df_wmt16_en.copy(deep=True), words, confusing_letters, p1=0, p2=p2)\n",
    "    df_wmt16_en_temp.to_csv(\"data/injected/wmt16/wmt16_english_data_injection_swap_p2_\" + str(p2) + \".csv\", header=False, index=False)\n",
    "    df_swap_results.loc[len(df_swap_results)] = {\"dataset\":\"wmt16_en\", \"p1\":0, \"p2\":p2, \"words_swapped\":results[0], \"letters_swapped\":results[1], \"words_changed\":results[2], \"sentences_changed\":results[3]}\n",
    "    df_wmt16_en_temp.to_excel(\"data/injected/for_amazon/wmt16_english_data_injection_swap_p2_\" + str(p2) + \".xlsx\", header=False, index=False)\n",
    "\n",
    "    df_wmt16_en_temp, results = injection_swap_df(df_wmt16_en.copy(deep=True), words, confusing_letters, p1=p1, p2=p2)\n",
    "    df_wmt16_en_temp.to_csv(\"data/injected/wmt16/wmt16_english_data_injection_swap_p1_\" + str(p1) + \"_p2_\" + str(p2) + \".csv\", header=False, index=False)\n",
    "    df_swap_results.loc[len(df_swap_results)] = {\"dataset\":\"wmt16_en\", \"p1\":p1, \"p2\":p2, \"words_swapped\":results[0], \"letters_swapped\":results[1], \"words_changed\":results[2], \"sentences_changed\":results[3]}\n",
    "    df_wmt16_en_temp.to_excel(\"data/injected/for_amazon/wmt16_english_data_injection_swap_p1_\" + str(p1) + \"_p2_\" + str(p2) + \".xlsx\", header=False, index=False)\n",
    "    \n",
    "\n",
    "    \n",
    "df_swap_results.to_csv(\"data/injected/wmt14_wmt16_english_data_injection_swap_results.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a sentence that has some words that will be swapped with homophones and some letters that will be swapped with confusing letters.\n",
      "I am a sentence that has some words that vill be swapped with homophones and some letters that will be swapped with confusong letters.\n",
      "Words swapped: 0\n",
      "Letters swapped: 2\n",
      "Words changed: 2\n"
     ]
    }
   ],
   "source": [
    "#lets test the injection_swap function on a sentence with p2 = 0.025\n",
    "sentence = \"I am a sentence that has some words that will be swapped with homophones and some letters that will be swapped with confusing letters.\"\n",
    "print(sentence)\n",
    "\n",
    "sentence, words_swapped, letters_swapped, words_changed = injection_swap(sentence, words, confusing_letters, p1=0, p2=0.025)\n",
    "print(sentence)\n",
    "print(\"Words swapped: \" + str(words_swapped))\n",
    "print(\"Letters swapped: \" + str(letters_swapped))\n",
    "print(\"Words changed: \" + str(words_changed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spectacular Wingsuit Jump Over Bogota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sportsman Jhonathan Florez jumped from a helic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wearing a wingsuit, he flew past over the famo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A black box in your car?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As America's road planners struggle to find th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0              Spectacular Wingsuit Jump Over Bogota\n",
       "1  Sportsman Jhonathan Florez jumped from a helic...\n",
       "2  Wearing a wingsuit, he flew past over the famo...\n",
       "3                           A black box in your car?\n",
       "4  As America's road planners struggle to find th..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wmt14_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Testing\".islower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gerge\\anaconda3\\envs\\Dyslexia_Research\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud import translate\n",
    "import pickle\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "import requests, uuid, json\n",
    "import pandas as pd\n",
    "#need to set os environment variable with google applioation credentials\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"your key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our translation function from google api advanced translation v3\n",
    "def translate_text(source_lang=\"en-US\", target_lang=\"fr\", text=\"YOUR_TEXT_TO_TRANSLATE\", project_id=\"YOUR_PROJECT_ID\"):\n",
    "    \"\"\"Translating Text.\"\"\"\n",
    "    #this uses google credentials from the environment variable\n",
    "    client = translate.TranslationServiceClient()\n",
    "\n",
    "    location = \"global\"\n",
    "\n",
    "    parent = f\"projects/{project_id}/locations/{location}\"\n",
    "\n",
    "    # Translate text from English to French\n",
    "    # Detail on supported types can be found here:\n",
    "    # https://cloud.google.com/translate/docs/supported-formats\n",
    "    response = client.translate_text(\n",
    "        request={\n",
    "            \"parent\": parent,\n",
    "            \"contents\": [text],\n",
    "            \"mime_type\": \"text/plain\",  # mime types: text/plain, text/html\n",
    "            \"source_language_code\": source_lang, #was \"en-US\"\n",
    "            \"target_language_code\": target_lang, #was \"fr\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return response.translations[0].translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spectaculaire saut en \"wingsuit\" au-dessus de Bogota',\n",
       " \"Le sportif Jhonathan Florez a sauté jeudi d'un hélicoptère au-dessus de Bogota, la capitale colombienne.\",\n",
       " \"Equipé d'un wingsuit (une combinaison munie d'ailes), il est passé à 160 km/h au-dessus du célèbre sanctuaire Monserrate, situé à plus de 3 000 mètres d'altitude, où de nombreux badauds s'étaient rassemblés pour observer son exploit.\",\n",
       " 'Une boîte noire dans votre voiture\\xa0?',\n",
       " \"Alors que les planificateurs du réseau routier des États-Unis ont du mal à trouver l'argent nécessaire pour réparer l'infrastructure autoroutière en décrépitude, nombreux sont ceux qui entrevoient une solution sous forme d'une petite boîte noire qui se fixe au-dessus du tableau de bord de votre voiture.\"]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a results for the bleu scores of each file\n",
    "df_bleu_results = pd.DataFrame(columns=[\"dataset_name\", \"bleu_score\"])\n",
    "\n",
    "df_wmt14_reference = pd.read_csv('data/wmt14_french_data.csv', header=None)\n",
    "df_wmt14_reference.head()\n",
    "#convert to list\n",
    "wmt14_reference = df_wmt14_reference[0].tolist()\n",
    "wmt14_reference[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wmt14_english_data_injection_swap_p1_0.005.csv bleu score: {'bleu': 0.4558755588054787, 'precisions': [0.7186877163537186, 0.5224061915616768, 0.3974921005437103, 0.3056633510266589], 'brevity_penalty': 0.9864298508784737, 'length_ratio': 0.986521097974284, 'translation_length': 76264, 'reference_length': 77306}\n",
      "1/30 files\n",
      "wmt14_english_data_injection_swap_p1_0.005_p2_0.005.csv bleu score: {'bleu': 0.45097839852677496, 'precisions': [0.715205222657735, 0.5175354803493449, 0.39213398409152356, 0.30068672334859387], 'brevity_penalty': 0.986678931815459, 'length_ratio': 0.9867668744987452, 'translation_length': 76283, 'reference_length': 77306}\n",
      "2/30 files\n",
      "wmt14_english_data_injection_swap_p1_0.01.csv bleu score: {'bleu': 0.45564322922873857, 'precisions': [0.7185250265542428, 0.5221961341050563, 0.3972784080395143, 0.30547790400285496], 'brevity_penalty': 0.986364293089946, 'length_ratio': 0.9864564199415311, 'translation_length': 76259, 'reference_length': 77306}\n",
      "3/30 files\n",
      "wmt14_english_data_injection_swap_p1_0.015.csv bleu score: {'bleu': 0.4550002306803164, 'precisions': [0.7175625548870801, 0.5212853049529267, 0.3964602273535647, 0.30477365276580565], 'brevity_penalty': 0.9868100024629199, 'length_ratio': 0.9868962305642511, 'translation_length': 76293, 'reference_length': 77306}\n",
      "4/30 files\n",
      "wmt14_english_data_injection_swap_p1_0.015_p2_0.015.csv bleu score: {'bleu': 0.44385412498599847, 'precisions': [0.7100577762056361, 0.5102692087390557, 0.3845541288056539, 0.2931879623302932], 'brevity_penalty': 0.9872817164573058, 'length_ratio': 0.9873619124000724, 'translation_length': 76329, 'reference_length': 77306}\n",
      "5/30 files\n",
      "wmt14_english_data_injection_swap_p1_0.01_p2_0.01.csv bleu score: {'bleu': 0.4471951034400666, 'precisions': [0.7122800572080878, 0.5142466876109821, 0.3890067941943111, 0.2972502455137934], 'brevity_penalty': 0.9857609624846201, 'length_ratio': 0.9858613820402039, 'translation_length': 76213, 'reference_length': 77306}\n",
      "6/30 files\n",
      "wmt14_english_data_injection_swap_p1_0.02.csv bleu score: {'bleu': 0.45403087682282234, 'precisions': [0.7175560508719024, 0.5207392141073062, 0.39549413639986336, 0.30361417123827367], 'brevity_penalty': 0.9865085146289597, 'length_ratio': 0.9865987116135876, 'translation_length': 76270, 'reference_length': 77306}\n",
      "7/30 files\n",
      "wmt14_english_data_injection_swap_p1_0.025.csv bleu score: {'bleu': 0.4539161277554012, 'precisions': [0.7173565417988308, 0.5202139416845178, 0.3952595037559754, 0.3034910160952338], 'brevity_penalty': 0.9868231085953426, 'length_ratio': 0.9869091661708017, 'translation_length': 76294, 'reference_length': 77306}\n",
      "8/30 files\n",
      "wmt14_english_data_injection_swap_p1_0.025_p2_0.025.csv bleu score: {'bleu': 0.4346957925443079, 'precisions': [0.7027242855459699, 0.5010492750463317, 0.3749307341469999, 0.2838082517067379], 'brevity_penalty': 0.9880412382909134, 'length_ratio': 0.9881121775800067, 'translation_length': 76387, 'reference_length': 77306}\n",
      "9/30 files\n",
      "wmt14_english_data_injection_swap_p1_0.02_p2_0.02.csv bleu score: {'bleu': 0.4383987386556585, 'precisions': [0.705958600102125, 0.504715566822035, 0.378565033891802, 0.28750185542526346], 'brevity_penalty': 0.987910326895781, 'length_ratio': 0.9879828215145008, 'translation_length': 76377, 'reference_length': 77306}\n",
      "10/30 files\n",
      "wmt14_english_data_injection_swap_p1_0.03.csv bleu score: {'bleu': 0.45411313241796014, 'precisions': [0.7173235155390179, 0.5204784441019626, 0.3952899684291362, 0.30337061960574596], 'brevity_penalty': 0.9872162137578515, 'length_ratio': 0.9872972343673195, 'translation_length': 76324, 'reference_length': 77306}\n",
      "11/30 files\n",
      "wmt14_english_data_injection_swap_p1_0.035.csv bleu score: {'bleu': 0.4536821107020975, 'precisions': [0.7164937650633972, 0.519627493489317, 0.3947652762375956, 0.30314685833939736], 'brevity_penalty': 0.987478199142208, 'length_ratio': 0.9875559464983313, 'translation_length': 76344, 'reference_length': 77306}\n",
      "12/30 files\n",
      "wmt14_english_data_injection_swap_p1_0.035_p2_0.035.csv bleu score: {'bleu': 0.42592387107290547, 'precisions': [0.6965451256913681, 0.4919298724333174, 0.3664855175553406, 0.2762925205450952], 'brevity_penalty': 0.9868755314300055, 'length_ratio': 0.9869609085970041, 'translation_length': 76298, 'reference_length': 77306}\n",
      "13/30 files\n",
      "wmt14_english_data_injection_swap_p1_0.03_p2_0.03.csv bleu score: {'bleu': 0.4295103512860583, 'precisions': [0.6989572689879221, 0.4957114611031567, 0.3697890007393505, 0.2794403766467155], 'brevity_penalty': 0.9873996106424012, 'length_ratio': 0.9874783328590278, 'translation_length': 76338, 'reference_length': 77306}\n",
      "14/30 files\n",
      "wmt14_english_data_injection_swap_p1_0.04.csv bleu score: {'bleu': 0.4539160635792897, 'precisions': [0.7173693660633543, 0.5202734044585118, 0.3951205633402091, 0.3034445864415847], 'brevity_penalty': 0.986914846776284, 'length_ratio': 0.9869997154166559, 'translation_length': 76301, 'reference_length': 77306}\n",
      "15/30 files\n",
      "wmt14_english_data_injection_swap_p1_0.045.csv bleu score: {'bleu': 0.4528018199653724, 'precisions': [0.7161330049261084, 0.518799863620866, 0.39393077557521117, 0.3023276540752514], 'brevity_penalty': 0.9872686162562838, 'length_ratio': 0.9873489767935218, 'translation_length': 76328, 'reference_length': 77306}\n",
      "16/30 files\n",
      "wmt14_english_data_injection_swap_p1_0.045_p2_0.045.csv bleu score: {'bleu': 0.4158383425959777, 'precisions': [0.6883120278148119, 0.48140892208481284, 0.3553758865248227, 0.26477429295248817], 'brevity_penalty': 0.9895977849778552, 'length_ratio': 0.9896515147595271, 'translation_length': 76506, 'reference_length': 77306}\n",
      "17/30 files\n",
      "wmt14_english_data_injection_swap_p1_0.04_p2_0.04.csv bleu score: {'bleu': 0.4217583060950915, 'precisions': [0.6929342132830505, 0.4875212976214816, 0.3619709502288167, 0.27178931429165243], 'brevity_penalty': 0.9877924921627022, 'length_ratio': 0.9878664010555455, 'translation_length': 76368, 'reference_length': 77306}\n",
      "18/30 files\n",
      "wmt14_english_data_injection_swap_p1_0.05.csv bleu score: {'bleu': 0.45273954474800443, 'precisions': [0.7158162196144174, 0.5188209791544534, 0.39378216245415515, 0.3020120276189769], 'brevity_penalty': 0.9875829743223988, 'length_ratio': 0.9876594313507361, 'translation_length': 76352, 'reference_length': 77306}\n",
      "19/30 files\n",
      "wmt14_english_data_injection_swap_p1_0.05_p2_0.05.csv bleu score: {'bleu': 0.4109488685813045, 'precisions': [0.6835093866025205, 0.476288968417042, 0.3505235082143972, 0.26079869600651995], 'brevity_penalty': 0.9894147861611059, 'length_ratio': 0.9894704162678188, 'translation_length': 76492, 'reference_length': 77306}\n",
      "20/30 files\n",
      "wmt14_english_data_injection_swap_p2_0.005.csv bleu score: {'bleu': 0.4513359864649549, 'precisions': [0.7156740593072515, 0.5181038719895145, 0.3928047721416267, 0.30117489589530044], 'brevity_penalty': 0.986206937099867, 'length_ratio': 0.9863011926629239, 'translation_length': 76247, 'reference_length': 77306}\n",
      "21/30 files\n",
      "wmt14_english_data_injection_swap_p2_0.01.csv bleu score: {'bleu': 0.44862826118318977, 'precisions': [0.7136599695745686, 0.5155292222419419, 0.3901289753153204, 0.298267529184326], 'brevity_penalty': 0.9862725050636101, 'length_ratio': 0.9863658706956769, 'translation_length': 76252, 'reference_length': 77306}\n",
      "22/30 files\n",
      "wmt14_english_data_injection_swap_p2_0.015.csv bleu score: {'bleu': 0.44425110459761225, 'precisions': [0.7101804410781911, 0.5104760605647252, 0.3851252364629411, 0.2938816747392506], 'brevity_penalty': 0.9870720929078808, 'length_ratio': 0.9871549426952629, 'translation_length': 76313, 'reference_length': 77306}\n",
      "23/30 files\n",
      "wmt14_english_data_injection_swap_p2_0.02.csv bleu score: {'bleu': 0.44000777328816254, 'precisions': [0.7063286163522012, 0.5056944501275284, 0.3806638791705777, 0.290300536300566], 'brevity_penalty': 0.9871638085483653, 'length_ratio': 0.9872454919411171, 'translation_length': 76320, 'reference_length': 77306}\n",
      "24/30 files\n",
      "wmt14_english_data_injection_swap_p2_0.025.csv bleu score: {'bleu': 0.4357005561886606, 'precisions': [0.7048232270247587, 0.5023138984068912, 0.3763131672597865, 0.285803506371842], 'brevity_penalty': 0.9863249563818929, 'length_ratio': 0.9864176131218793, 'translation_length': 76256, 'reference_length': 77306}\n",
      "25/30 files\n",
      "wmt14_english_data_injection_swap_p2_0.03.csv bleu score: {'bleu': 0.43420514255436815, 'precisions': [0.7021831770799187, 0.500586934563621, 0.37558177600022774, 0.2843527907460823], 'brevity_penalty': 0.9864429619274643, 'length_ratio': 0.9865340335808346, 'translation_length': 76265, 'reference_length': 77306}\n",
      "26/30 files\n",
      "wmt14_english_data_injection_swap_p2_0.035.csv bleu score: {'bleu': 0.4269522730926865, 'precisions': [0.6977694534309552, 0.4939055981239604, 0.3671154201555237, 0.27614267470523596], 'brevity_penalty': 0.9875436849002442, 'length_ratio': 0.9876206245310842, 'translation_length': 76349, 'reference_length': 77306}\n",
      "27/30 files\n",
      "wmt14_english_data_injection_swap_p2_0.04.csv bleu score: {'bleu': 0.4216965733766452, 'precisions': [0.6941664920368819, 0.48863651856194357, 0.36179740141585875, 0.2708887074021828], 'brevity_penalty': 0.9875829743223988, 'length_ratio': 0.9876594313507361, 'translation_length': 76352, 'reference_length': 77306}\n",
      "28/30 files\n",
      "wmt14_english_data_injection_swap_p2_0.045.csv bleu score: {'bleu': 0.4191672708792469, 'precisions': [0.6912775399599419, 0.48547406862344317, 0.35923447423383487, 0.26866225401442523], 'brevity_penalty': 0.9880674185381954, 'length_ratio': 0.988138048793108, 'translation_length': 76389, 'reference_length': 77306}\n",
      "29/30 files\n",
      "wmt14_english_data_injection_swap_p2_0.05.csv bleu score: {'bleu': 0.4138597492719917, 'precisions': [0.687478725354141, 0.4803417871598141, 0.35357224053654657, 0.2637179962894249], 'brevity_penalty': 0.9879757847098104, 'length_ratio': 0.9880474995472538, 'translation_length': 76382, 'reference_length': 77306}\n",
      "30/30 files\n"
     ]
    }
   ],
   "source": [
    "#for every csv in the \"C:\\Users\\gerge\\OneDrive\\Desktop\\AImpower\\data\\injected\\wmt14\" folder \n",
    "#we run it through the google api translation\n",
    "#we loop through the csvs in the folder\n",
    "counter = 1\n",
    "for filename in os.listdir(\"C:\\\\Users\\\\gerge\\\\OneDrive\\\\Desktop\\\\AImpower\\\\data\\\\injected\\\\wmt14\"):\n",
    "    #we open the csv with pandas and convert it to a list\n",
    "    df = pd.read_csv(\"C:\\\\Users\\\\gerge\\\\OneDrive\\\\Desktop\\\\AImpower\\\\data\\\\injected\\\\wmt14\\\\\" + filename, header=None)\n",
    "    df = df[0].tolist()\n",
    "    candidate_corpus = []\n",
    "    for i in range(len(df)):\n",
    "        candidate_corpus.append(translate_text(target_lang=\"fr\",text=df[i], project_id=\"dyslexia-translation\"))\n",
    "\n",
    "    #calculate the bleu score\n",
    "    bleu_score = bleu.compute(predictions=candidate_corpus, references=wmt14_reference)\n",
    "    #save the candidate corpus to pickle file\n",
    "    with open(\"C:\\\\Users\\\\gerge\\\\OneDrive\\\\Desktop\\\\AImpower\\\\data\\\\injected\\\\\\google_candidate_corpus\\\\wmt14\\\\\" + filename[:-4] + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(candidate_corpus, f)\n",
    "    \n",
    "    #add the bleu score to the results dataframe\n",
    "    df_bleu_results.loc[len(df_bleu_results)] = {\"dataset_name\":filename, \"bleu_score\":bleu_score}\n",
    "    #print the results and filename\n",
    "    print(filename + \" bleu score: \" + str(bleu_score))\n",
    "\n",
    "    #print the number of files left to process\n",
    "    print(str(counter) + \"/\"+ str(len(os.listdir(\"C:\\\\Users\\\\gerge\\\\OneDrive\\\\Desktop\\\\AImpower\\\\data\\\\injected\\\\wmt14\"))) + \" files\")\n",
    "    counter += 1\n",
    "    \n",
    "\n",
    "\n",
    "df_bleu_results.to_csv(\"data/injected/wmt14_bleu_results.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Obama empfängt Netanyahu',\n",
       " 'Das Verhältnis zwischen Obama und Netanyahu ist nicht gerade freundschaftlich.',\n",
       " 'Die beiden wollten über die Umsetzung der internationalen Vereinbarung sowie über Teherans destabilisierende Maßnahmen im Nahen Osten sprechen.',\n",
       " 'Bei der Begegnung soll es aber auch um den Konflikt mit den Palästinensern und die diskutierte Zwei-Staaten-Lösung gehen.',\n",
       " 'Das Verhältnis zwischen Obama und Netanyahu ist seit Jahren gespannt.']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do exactly the same for wmt16\n",
    "df_bleu_results_wmt16_google = pd.DataFrame(columns=[\"dataset_name\", \"bleu_score\"])\n",
    "\n",
    "df_wmt16_reference = pd.read_csv('data/wmt16_german_data.csv', header=None)\n",
    "df_wmt16_reference.head()\n",
    "#convert to list\n",
    "wmt16_reference = df_wmt16_reference[0].tolist()\n",
    "wmt16_reference[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wmt16_english_data_injection_swap_p1_0.005.csv bleu score: {'bleu': 0.3957579447636291, 'precisions': [0.6990445598911814, 0.4687004481213375, 0.3366351029606877, 0.24781839849306117], 'brevity_penalty': 0.9733215702731506, 'length_ratio': 0.9736711930938742, 'translation_length': 61019, 'reference_length': 62669}\n",
      "1/30 files\n",
      "wmt16_english_data_injection_swap_p1_0.005_p2_0.005.csv bleu score: {'bleu': 0.39019375319766253, 'precisions': [0.6942975490757871, 0.4628099173553719, 0.33062217461556614, 0.24213800253427026], 'brevity_penalty': 0.9743040474194508, 'length_ratio': 0.9746286042540969, 'translation_length': 61079, 'reference_length': 62669}\n",
      "2/30 files\n",
      "wmt16_english_data_injection_swap_p1_0.01.csv bleu score: {'bleu': 0.3952790545504284, 'precisions': [0.6987681420568096, 0.46808620600547834, 0.3359795087923267, 0.2470750965362227], 'brevity_penalty': 0.9737638013122918, 'length_ratio': 0.9741020281159743, 'translation_length': 61046, 'reference_length': 62669}\n",
      "3/30 files\n",
      "wmt16_english_data_injection_swap_p1_0.015.csv bleu score: {'bleu': 0.3952996264714369, 'precisions': [0.6987661603499975, 0.46834396002067896, 0.336192327960604, 0.2471173802751941], 'brevity_penalty': 0.9734853817465948, 'length_ratio': 0.9738307616205779, 'translation_length': 61029, 'reference_length': 62669}\n",
      "4/30 files\n",
      "wmt16_english_data_injection_swap_p1_0.015_p2_0.015.csv bleu score: {'bleu': 0.3836665330668471, 'precisions': [0.689640488084514, 0.45643861099627947, 0.32435475961276494, 0.2358918212900004], 'brevity_penalty': 0.9739111693541944, 'length_ratio': 0.9742456397900078, 'translation_length': 61055, 'reference_length': 62669}\n",
      "5/30 files\n",
      "wmt16_english_data_injection_swap_p1_0.01_p2_0.01.csv bleu score: {'bleu': 0.3901127693295175, 'precisions': [0.6936730791232859, 0.4620514230716348, 0.3296729183654979, 0.24155968576355624], 'brevity_penalty': 0.9760047826151372, 'length_ratio': 0.9762881169318164, 'translation_length': 61183, 'reference_length': 62669}\n",
      "6/30 files\n",
      "wmt16_english_data_injection_swap_p1_0.02.csv bleu score: {'bleu': 0.39556591486608, 'precisions': [0.6987470313651626, 0.4682892379771255, 0.33636049911909477, 0.24726287887518728], 'brevity_penalty': 0.9739111693541944, 'length_ratio': 0.9742456397900078, 'translation_length': 61055, 'reference_length': 62669}\n",
      "7/30 files\n",
      "wmt16_english_data_injection_swap_p1_0.025.csv bleu score: {'bleu': 0.39442981367117824, 'precisions': [0.6980485966608229, 0.4673984216149154, 0.3351866993731262, 0.24636817832436586], 'brevity_penalty': 0.9735508990226576, 'length_ratio': 0.9738945890312595, 'translation_length': 61033, 'reference_length': 62669}\n",
      "8/30 files\n",
      "wmt16_english_data_injection_swap_p1_0.025_p2_0.025.csv bleu score: {'bleu': 0.37298045040879924, 'precisions': [0.6796021522908854, 0.44348631088100665, 0.3114472637894889, 0.22498423677321971], 'brevity_penalty': 0.9784039002132922, 'length_ratio': 0.9786337742743622, 'translation_length': 61330, 'reference_length': 62669}\n",
      "9/30 files\n",
      "wmt16_english_data_injection_swap_p1_0.02_p2_0.02.csv bleu score: {'bleu': 0.37946255976794174, 'precisions': [0.6843609169877866, 0.4500815520645549, 0.3187380081815878, 0.23178503760693575], 'brevity_penalty': 0.9770010175882695, 'length_ratio': 0.9772614849447095, 'translation_length': 61244, 'reference_length': 62669}\n",
      "10/30 files\n",
      "wmt16_english_data_injection_swap_p1_0.03.csv bleu score: {'bleu': 0.3937484833547318, 'precisions': [0.6974610974610974, 0.4665897228299254, 0.3343747729419458, 0.24561538313771442], 'brevity_penalty': 0.9738293008307856, 'length_ratio': 0.9741658555266559, 'translation_length': 61050, 'reference_length': 62669}\n",
      "11/30 files\n",
      "wmt16_english_data_injection_swap_p1_0.035.csv bleu score: {'bleu': 0.39491064467025155, 'precisions': [0.697927240577622, 0.4679660462473527, 0.3355119825708061, 0.24632811750024], 'brevity_penalty': 0.9742876805017744, 'length_ratio': 0.9746126474014265, 'translation_length': 61078, 'reference_length': 62669}\n",
      "12/30 files\n",
      "wmt16_english_data_injection_swap_p1_0.035_p2_0.035.csv bleu score: {'bleu': 0.3677583805420395, 'precisions': [0.6752748866194656, 0.43789773409492444, 0.30656419529837253, 0.220663021450694], 'brevity_penalty': 0.9778821221462666, 'length_ratio': 0.97812315498891, 'translation_length': 61298, 'reference_length': 62669}\n",
      "13/30 files\n",
      "wmt16_english_data_injection_swap_p1_0.03_p2_0.03.csv bleu score: {'bleu': 0.37178592115022974, 'precisions': [0.6784011495003592, 0.4426646064039832, 0.3111537486876878, 0.22441675757401772], 'brevity_penalty': 0.9770010175882695, 'length_ratio': 0.9772614849447095, 'translation_length': 61244, 'reference_length': 62669}\n",
      "14/30 files\n",
      "wmt16_english_data_injection_swap_p1_0.04.csv bleu score: {'bleu': 0.3942010630782961, 'precisions': [0.6974722504174716, 0.4668835976103163, 0.33481591750780626, 0.24573326422085276], 'brevity_penalty': 0.9743531466065828, 'length_ratio': 0.9746764748121081, 'translation_length': 61082, 'reference_length': 62669}\n",
      "15/30 files\n",
      "wmt16_english_data_injection_swap_p1_0.045.csv bleu score: {'bleu': 0.393846023414038, 'precisions': [0.6966990545359375, 0.46596714543734413, 0.3338834881021474, 0.24543065917416237], 'brevity_penalty': 0.9752038260831496, 'length_ratio': 0.9755062311509678, 'translation_length': 61134, 'reference_length': 62669}\n",
      "16/30 files\n",
      "wmt16_english_data_injection_swap_p1_0.045_p2_0.045.csv bleu score: {'bleu': 0.3602995451425025, 'precisions': [0.6686070211482333, 0.4310989199382822, 0.29853066093148506, 0.21373433200856007], 'brevity_penalty': 0.9783875986792966, 'length_ratio': 0.9786178174216917, 'translation_length': 61329, 'reference_length': 62669}\n",
      "17/30 files\n",
      "wmt16_english_data_injection_swap_p1_0.04_p2_0.04.csv bleu score: {'bleu': 0.36030386935573205, 'precisions': [0.6687956204379562, 0.43003237576442777, 0.2988009678933873, 0.21335165988965885], 'brevity_penalty': 0.9791534897865666, 'length_ratio': 0.9793677894971996, 'translation_length': 61376, 'reference_length': 62669}\n",
      "18/30 files\n",
      "wmt16_english_data_injection_swap_p1_0.05.csv bleu score: {'bleu': 0.39360658496800216, 'precisions': [0.6969314906995022, 0.46622354622630136, 0.3341867305806733, 0.2454156185794658], 'brevity_penalty': 0.9741894735148022, 'length_ratio': 0.9745169062854042, 'translation_length': 61072, 'reference_length': 62669}\n",
      "19/30 files\n",
      "wmt16_english_data_injection_swap_p1_0.05_p2_0.05.csv bleu score: {'bleu': 0.35498037923531717, 'precisions': [0.6631740186093503, 0.4237938596491228, 0.2936841915151077, 0.20941688624126475], 'brevity_penalty': 0.9790068742279622, 'length_ratio': 0.9792241778231662, 'translation_length': 61367, 'reference_length': 62669}\n",
      "20/30 files\n",
      "wmt16_english_data_injection_swap_p2_0.005.csv bleu score: {'bleu': 0.3910151112286825, 'precisions': [0.6951323375262054, 0.4637166922162702, 0.33170474772058556, 0.2429940648829303], 'brevity_penalty': 0.9739275422756211, 'length_ratio': 0.9742615966426782, 'translation_length': 61056, 'reference_length': 62669}\n",
      "21/30 files\n",
      "wmt16_english_data_injection_swap_p2_0.01.csv bleu score: {'bleu': 0.3876442978535413, 'precisions': [0.6920699298412023, 0.4598094517438261, 0.32771219786396855, 0.23919545960041416], 'brevity_penalty': 0.9754163857557828, 'length_ratio': 0.9757136702356827, 'translation_length': 61147, 'reference_length': 62669}\n",
      "22/30 files\n",
      "wmt16_english_data_injection_swap_p2_0.015.csv bleu score: {'bleu': 0.3855203932177606, 'precisions': [0.6903337582949234, 0.4580203839609508, 0.325529138880835, 0.23652493820536893], 'brevity_penalty': 0.9759884428188611, 'length_ratio': 0.976272160079146, 'translation_length': 61182, 'reference_length': 62669}\n",
      "23/30 files\n",
      "wmt16_english_data_injection_swap_p2_0.02.csv bleu score: {'bleu': 0.3811647414461645, 'precisions': [0.6865334577053366, 0.4533971225992979, 0.32143570015786893, 0.2336800092105768], 'brevity_penalty': 0.9747622151686764, 'length_ratio': 0.9750753961288675, 'translation_length': 61107, 'reference_length': 62669}\n",
      "24/30 files\n",
      "wmt16_english_data_injection_swap_p2_0.025.csv bleu score: {'bleu': 0.3730826729083473, 'precisions': [0.6813382583682008, 0.44496004124774424, 0.3126336389664045, 0.22524956410109023], 'brevity_penalty': 0.9760211221508377, 'length_ratio': 0.9763040737844868, 'translation_length': 61184, 'reference_length': 62669}\n",
      "25/30 files\n",
      "wmt16_english_data_injection_swap_p2_0.03.csv bleu score: {'bleu': 0.3720747745509314, 'precisions': [0.6798973789136545, 0.44419815454404865, 0.31149679336207836, 0.22431661015650442], 'brevity_penalty': 0.9762171762557617, 'length_ratio': 0.9764955560165313, 'translation_length': 61196, 'reference_length': 62669}\n",
      "26/30 files\n",
      "wmt16_english_data_injection_swap_p2_0.035.csv bleu score: {'bleu': 0.36924969456274387, 'precisions': [0.6743924319034416, 0.43933391641371267, 0.30828753254266705, 0.2223942504348491], 'brevity_penalty': 0.9780778201290443, 'length_ratio': 0.9783146372209546, 'translation_length': 61310, 'reference_length': 62669}\n",
      "27/30 files\n",
      "wmt16_english_data_injection_swap_p2_0.04.csv bleu score: {'bleu': 0.36355109832715876, 'precisions': [0.6729908892009274, 0.4336875718921146, 0.3022733854619172, 0.21728895948557977], 'brevity_penalty': 0.9770336646240082, 'length_ratio': 0.9772933986500503, 'translation_length': 61246, 'reference_length': 62669}\n",
      "28/30 files\n",
      "wmt16_english_data_injection_swap_p2_0.045.csv bleu score: {'bleu': 0.36508627207994243, 'precisions': [0.6715887027035846, 0.4354606278950077, 0.30408203866813766, 0.21860180510937738], 'brevity_penalty': 0.9777353240780561, 'length_ratio': 0.9779795433148766, 'translation_length': 61289, 'reference_length': 62669}\n",
      "29/30 files\n",
      "wmt16_english_data_injection_swap_p2_0.05.csv bleu score: {'bleu': 0.35438859742410606, 'precisions': [0.6639217537064462, 0.424668081029291, 0.2922866057097513, 0.20724825561444313], 'brevity_penalty': 0.9803093860015746, 'length_ratio': 0.9805007260367965, 'translation_length': 61447, 'reference_length': 62669}\n",
      "30/30 files\n"
     ]
    }
   ],
   "source": [
    "#same as above but for wmt16\n",
    "counter = 1\n",
    "for filename in os.listdir(\"C:\\\\Users\\\\gerge\\\\OneDrive\\\\Desktop\\\\AImpower\\\\data\\\\injected\\\\wmt16\"):\n",
    "    df = pd.read_csv(\"C:\\\\Users\\\\gerge\\\\OneDrive\\\\Desktop\\\\AImpower\\\\data\\\\injected\\\\wmt16\\\\\" + filename, header=None)\n",
    "    df = df[0].tolist()\n",
    "    candidate_corpus = []\n",
    "    for i in range(len(df)):\n",
    "        candidate_corpus.append(translate_text(target_lang=\"de\",text=df[i], project_id=\"dyslexia-translation\"))\n",
    "\n",
    "    bleu_score = bleu.compute(predictions=candidate_corpus, references=wmt16_reference)\n",
    "    with open(\"C:\\\\Users\\\\gerge\\\\OneDrive\\\\Desktop\\\\AImpower\\\\data\\\\injected\\\\\\google_candidate_corpus\\\\wmt16\\\\\" + filename[:-4] + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(candidate_corpus, f)\n",
    "    df_bleu_results_wmt16_google.loc[len(df_bleu_results_wmt16_google)] = {\"dataset_name\":filename, \"bleu_score\":bleu_score}\n",
    "    print(filename + \" bleu score: \" + str(bleu_score))\n",
    "    print(str(counter) + \"/\"+ str(len(os.listdir(\"C:\\\\Users\\\\gerge\\\\OneDrive\\\\Desktop\\\\AImpower\\\\data\\\\injected\\\\wmt16\"))) + \" files\")\n",
    "    counter += 1\n",
    "\n",
    "df_bleu_results_wmt16_google.to_csv(\"data/injected/wmt16_bleu_results_google.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999\n"
     ]
    }
   ],
   "source": [
    "#print len of german reference\n",
    "print(len(wmt16_reference))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999\n"
     ]
    }
   ],
   "source": [
    "#print len of sample candidate corpus\n",
    "print(len(candidate_corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dyslexia_Research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
